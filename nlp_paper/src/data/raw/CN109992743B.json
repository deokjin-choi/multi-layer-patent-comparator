{
  "patent_id": "CN109992743B",
  "title": "Matrix multiplier",
  "assignee": "Huawei Technologies Co Ltd",
  "description": "Description.Translated from.Chinese..matrix multiplier..technical field..The invention relates to the technical field of computing, and in particular, to a matrix multiplier...Background technique.AB.Currently, to compute the product of two matrices A and B, it can be done in either of two ways:..Method 1: Calculation is performed by a vector processor..CABM1Ai(A.i1.A.i2.A.i(M-1).A.iM.)Reg0Bj(B.j1.B.j2.B.j(M-1).B.jM.)Reg1Reg0Reg1CijC.ij.C.Assuming.CA..B., the number of elements that can be calculated by the vector processor at the same time is M. Referring to Figure 1, the vector processor will.(M-1)., A.iM.) are loaded into the source register Reg0, and then the j-th column vector of matrix B (including elements B.j1., B.j2., ..., B.j(M-1)., B.jM.) is loaded into the register In Reg1, the multiplication between the corresponding elements of Reg0 and Reg1 can be realized, and finally the accumulation operation is completed through the addition tree, and the data C.ij.of the i-th row and the j-th column of the matrix C is calculated, and the matrix C can be obtained by performing multiple calculations. ...Mode 2: In order to further improve the calculation speed, the matrix multiplication operation can be completed through a two-dimensional calculation array..NNNNN3MN3/MNNN3N2N3/N2NNN.For example, the two-dimensional computational array may be an NN systolic array. In.mode.1, to complete two NN matrix multiplication operations, N3 multiplication operations are required. Since the vector processor can calculate the multiplication between M elements per clock cycle, the time required to complete one multiplication operation is is N3/M clock cycles. In.method.2, to complete two NN matrix multiplication operations, N3 multiplication operations are required. Since the systolic array has N2 operation units, the time required to complete one matrix operation is N3/N 2N clock cycles. The first method and the second method take a long time to complete the NN matrix multiplication operation, and both have the problem that the calculation size is relatively fixed and inflexible...SUMMARY OF THE INVENTION..The technical problem to be solved by the embodiments of the present invention is to provide a matrix multiplier and related equipment to solve the problems of inflexible calculation and low efficiency in matrix multiplication...In a first aspect, an embodiment of the present invention provides a matrix multiplier, which may include:.MK.a first memory for storing a first matrix, the first matrix being an MK matrix;.KN.a second memory for storing a second matrix, the second matrix being a KN matrix;.XY.an operation circuit connected to the first memory and the second memory, the operation circuit includes an X rowY column operation unit, each operation unit includes a vector multiplication circuit and an addition circuit, and the matrix multiplication circuit is used for receiving The row vector data sent by the first memory and the column vector data sent by the second memory are multiplied by the two-way vector; the adding circuit is configured to add the results of the two-way vector multiplication, and Accumulate the calculation results belonging to the same operation unit to obtain the operation result of each operation unit;..A controller connected to the arithmetic circuit, the controller is configured to perform the following actions:.XLSRSRsrA.sr.s(123S)r(123R).The first matrix is divided into sub-blocks with a scale of XL as a unit to obtain SR sub-blocks of the same size, wherein the s-th row and the r-th column of the SR sub-blocks are divided into blocks. The sub-block is denoted as A.sr., s(1, 2, 3,...S), r(1, 2, 3,...R);.LYRTRTrtB.rt.r(123R)t(123T).The second matrix is divided into sub-blocks with a scale of LY as a unit to obtain RT sub-blocks of the same size, wherein the r-th row and the t-th column in the RT sub-blocks are divided into blocks. Denoted as B.rt., r(1, 2, 3,...R), t(1, 2, 3,...T);..The controller is also used to perform the following actions:.A.sr.XxB.rt.YyXYxyx(123X)y(123Y)A.sr.rB.rt.r.Input the xth row in the X row vectors of any sub-block A.sr.and the yth column in the Y column vectors of the corresponding sub-block B.rt.into the xth row in the X rowY column operation unit The operation is performed in the operation unit of row y column, x(1, 2, 3,...X), y(1, 2, 3,...Y), wherein, in any one of the sub-blocks A.sr.The value of r is equal to the value of r in the corresponding sub-block B.rt...MNK60604(XL x LY)604603XYFC.This embodiment of the present invention provides a matrix multiplier. The matrix multiplier uses a controller to complete a block method for matrix multiplication, that is, MNK fractal. The control logic of the internal controller 604 in the matrix multiplier 60 is used. Split a large matrix into identity matrix multiplication (i.e. a matrix of XL x LY). The control logic of the controller 604 sends the unit matrix multiplication task to the operation circuit 603 every clock cycle, so that the data pipeline is executed, so that the X rowY column operation unit operates at full load. The efficiency of matrix multiplication is improved, and the application effect of neural network algorithm is significantly improved. The matrix multiplier provided by the embodiment of the present invention can perform the convolution operation and the FC operation in the convolutional neural network...In a possible implementation manner, the controller is specifically configured to perform the following actions:.A.sr.XxB.rt.YyXYxy.Input the x-th row in the X row vectors of any sub-block A.sr.and the y-th column in the Y column vectors of the corresponding sub-block B.rt.into the X rowY column in the same clock cycle The operation is performed in the operation unit of the xth row and the yth column of the operation unit..A.sr.xXYx1B.rt.yXYy1.In a possible implementation manner, the controller is further configured to control the row vector of any A.sr.to enter the first row corresponding to the X rowY column arithmetic unit in order of the x row number from small to large. x rows, and the time difference between adjacent row vectors entering the operation units of the same column and different rows is 1 clock cycle; the controller is also used to simultaneously control the column vectors of the corresponding sub-block B.rt.according to the y column numbers Enter the yth row corresponding to the X rowY column operation unit in order from small to large, and the time difference between adjacent column vectors entering the operation units of the same row and different columns is 1 clock cycle...In a possible implementation, the controller is also used to control:.srtA.sr.XYA.sr.B.rt..In at least two consecutive sub-block multiplication calculation cycles, the values of s and r are unchanged, and the value of t is changed, so that the first memory multiplies in the at least two consecutive sub-blocks The same A.sr.is multiplexed in the calculation cycle, wherein the sub-block multiplication calculation cycle is the time taken by the X rowY column operation unit to calculate and complete the matrix multiplication operation of one sub-block A.sr.and the corresponding sub-block B.rt....In a possible implementation manner, the matrix multiplier further includes a third memory connected to the operation circuit;.XY.The controller is used for controlling that the X rowY column operation unit stores the calculation results of the vector multiplication circuit and the addition circuit in the third memory...In a possible implementation manner, the matrix multiplier further includes a fourth memory connected to the first memory and the second memory, and a fifth memory connected to the third memory;..The controller is further configured to control, before computing the multiplication operation of the first matrix and the second matrix:..transfer the data sources of the first matrix and the second matrix from the fourth memory to the first memory and the second memory respectively; and transfer the calculation result from the third memory to the fifth memory storage..L(L1).In a possible implementation manner, the vector multiplication circuit includes L multipliers; the addition circuit includes an addition tree with an input number of (L1)...In a possible implementation manner, the first memory, the second memory, the arithmetic circuit and the controller are connected through a bus interface unit...In one possible implementation,.MX0(M1)(SX-M)0KY0(K1),RY-K0.When M%X0, the (M1)th to (SXM)th rows of the first matrix are not calculated, and the result is assigned as 0. When K%Y0, the first matrix The (K1)th, to the RYK row are not calculated, and the result is assigned as 0;..KY0(K1)(RY-K)0NX0(N1)(TX-N)0.In one possible implementation,.When K%Y0, the (K1)th to (RYK)th of the first matrix are not calculated, and the result is assigned as 0, when N%X0, the first matrix Lines (N1) to (TXN) of are not evaluated, and the result is assigned the.value.0...In a possible implementation manner, the matrix multiplier further includes a data handling unit, and the data handling unit is configured to transform the first matrix before moving the first matrix to the first memory. An operation of transposing a matrix is performed, or an operation of transposing a matrix is performed on the second matrix before transferring the second matrix to the second memory...In a possible implementation manner, the controller controls any sub-block of the first matrix to be stored in the first memory in the form of rows, or controls any sub-block of the second matrix to be stored in the first memory in the form of rows The row form is stored in the second memory. So that it can be read quickly when reading, and it is more flexible and fast when sub-blocks are transposed...In a second aspect, the present application provides an electronic device, which may include:..A security element and a discrete device coupled to the chip are provided by any one of the implementation manners of the above-mentioned first aspect...In a third aspect, the present application provides a system-on-chip chip, where the system-on-a-chip chip includes the chip provided by any one of the implementation manners of the first aspect above. The system-on-a-chip chip, which can consist of chips, can also contain chips and other discrete devices..Description of drawings..In order to more clearly describe the technical solutions in the embodiments of the present invention or the background technology, the accompanying drawings required in the embodiments or the background technology of the present invention will be described below..1.Fig. 1 is the process schematic diagram of calculating two matrix products in the prior art;.2.2 is a schematic diagram of converting a convolution kernel into a weight matrix in the prior art;.3.3 is a schematic diagram of converting input data into an input matrix in the prior art;.4.4 is a schematic diagram of a method for multiplying two matrices in the prior art;.5TPU.5 is a schematic diagram of a TPU systolic array in the prior art;.6.FIG. 6 is a structural diagram of a matrix multiplication accelerator provided by an embodiment of the present invention.76030.FIG. 7 is a structural diagram of a.computing unit.6030 according to an embodiment of the present invention;.8.8 is a schematic diagram of a matrix block according to an embodiment of the present invention;.9603.FIG. 9 is a schematic diagram of wiring in a specific arithmetic circuit 603 provided by an embodiment of the present invention;.10603.FIG. 10 is a schematic diagram of wiring in another specific arithmetic circuit 603 provided by an embodiment of the present invention;.11Base4.11 is an input format of a matrix multiplier whose Base is 4 provided by an embodiment of the present invention;.12T0M2N2K2.12 is a schematic diagram of the pipeline execution of the matrix multiplier of M2N2K2 at the time of T0;.13T1M2N2K2.13 is a schematic diagram of the pipeline execution of the M2N2K2 matrix multiplier at the moment of T1;.14T7M2N2K2.14 is a schematic diagram of the pipeline execution of the M2N2K2 matrix multiplier at time T7;.15T11M2N2K2.15 is a schematic diagram of the pipeline execution of the M2N2K2 matrix multiplier at time T11;.16.16 is a schematic structural diagram of another matrix multiplier provided by an embodiment of the present invention;.17.17 is a schematic structural diagram of another matrix multiplier provided by an embodiment of the present invention;.18.FIG. 18 is a schematic diagram of an asynchronous execution sequence of an instruction according to an embodiment of the present invention...Detailed ways..The embodiments of the present invention will be described below with reference to the accompanying drawings in the embodiments of the present invention...The terms \"first\", \"second\", \"third\" and \"fourth\" in the description and claims of the present application and the drawings are used to distinguish different objects, rather than to describe a specific order . Furthermore, the terms \"comprising\" and \"having\" and any variations thereof are intended to cover non-exclusive inclusion. For example, a process, method, system, product or device comprising a series of steps or units is not limited to the listed steps or units, but optionally also includes unlisted steps or units, or optionally also includes For other steps or units inherent to these processes, methods, products or devices...Reference herein to an \"embodiment\" means that a particular feature, structure, or characteristic described in connection with the embodiment can be included in at least one embodiment of the present application. The appearances of the phrase in various places in the specification are not necessarily all referring to the same embodiment, nor a separate or alternative embodiment that is mutually exclusive of other embodiments. It is explicitly and implicitly understood by those skilled in the art that the embodiments described herein may be combined with other embodiments..///2(/)/.The terms \"component\", \"module\", \"system\" and the like are used in this specification to refer to a computer-related entity, hardware, firmware, a combination of hardware and software, software, or software in execution. For example, a component may be, but is not limited to, a process running on a processor, a processor, an object, an executable, a thread of execution, a program, and/or a computer. By way of illustration, both an application running on a computing device and the computing device may be components. One or more components may reside within a process and/or thread of execution, and a component may be localized on one computer and/or distributed between 2 or more computers. In addition, these components can execute from various computer readable media having various data structures stored thereon. A component may, for example, be based on a signal having one or more data packets (eg, data from two components interacting with another component between a local system, a distributed system, and/or a network, such as the Internet interacting with other systems via signals) Communicate through local and/or remote processes..(fully connectedFC)70.Secondly, the technical problems and application scenarios to be solved in this application are proposed. In recent years, due to the good performance of convolutional neural network in image classification, image recognition, audio recognition and other related fields, it has become a research and development hotspot in academia and industry. The convolutional neural network mainly includes convolution and fully connected (fully connected, FC) operations, in which the operation volume of the convolution operation can usually occupy more than 70% of the total network operation volume..xyz(filter)2KKzNNK(weight matrix)().The convolution operation is not strictly equivalent to the matrix multiplication operation, but through reasonable data adjustment, the convolution operation can be converted into a matrix multiplication operation. In a convolutional neural network, there are usually multiple convolution kernels. The convolution kernel is three-dimensional and contains data in three dimensions. The x and y directions are the length and width of the data, and the z direction can be considered as the depth of the data. The convolution kernel is actually a filter, which is mainly used to extract different features in the image. Referring to Figure 2, the convolution kernel is essentially a combination of a series of weights. Assuming that the number of convolution kernels is K, N elements in the z direction at the same position in the K convolution kernels are extracted to obtain NK According to the specification of the matrix multiplier (that is, the number of rows and columns of the matrix that the matrix multiplier can calculate), the convolution kernel can be pre-stored in the memory of the matrix multiplier in the form of a weight matrix , to be called when the matrix multiplier does a matrix multiplication operation. \"\" in the embodiment of the present invention means \"multiplying\"..3(stride)(1)MzNMN(input matrix).Referring to FIG. 3 , according to the stride of the convolution kernel (the stride is 1 in the embodiment of the present invention), the matrix multiplier can extract N data of the input M points in the z direction, a total of MN data , an input matrix can be formed, and the matrix multiplier needs to multiply the input matrix and the weight matrix..FCFC9216FC4096FC91269216409692169216x40964CABAMKBKNMNKCABCNCMNK.The FC operation is essentially a vector-matrix multiplication operation. The input of the FC operation is a 9216 vector, FC needs to output 4096 points, then to get a point output by FC, a 9126 vector and 9216 weights are required to perform a dot product operation. To get all 4096 points, 9216 points are required. The vector is dot-multiplied with 9216x4096 weights. FIG. 4 shows the calculation formula of matrix CAB, where A is a matrix with size MK, and B is a matrix with size KN. In this embodiment of the present invention, M, N, and K are all is a positive integer. To calculate a data in the C matrix, the data in a row vector in the matrix A and the corresponding data in a column vector in the matrix B need to be dot-multiplied and then accumulated, that is, to calculate a C matrix. If the data needs to be multiplied by N times, it needs to perform MNK multiplications to obtain the matrix C..Google(ASIC)Google TPUv1TPU2562562-D MAC Array(5)Cell(Partial Sum,)256x2562D.In the prior art, the systolic array calculation method, such as Google TPUv1, a special-purpose chip (ASIC) customized by Google for machine learning, uses the design of the systolic array to realize a 256256 2-D MAC Array to optimize matrix multiplication. and convolution operation (as shown in Figure 5). Each Cell in the figure is a multiplier. When the multiplier multiplies an element in the two matrices, the calculated result (Partial Sum, that is, the intermediate result in the matrix multiplication) will be transmitted down to the figure. The accumulation unit below is accumulated with the last related accumulated value. Thus, when the data is full, the systolic array accumulates an intermediate value of the matrix size every clock cycle. In the above scheme, due to the low calculation density, the calculation efficiency of matrix multiplication is low; secondly, in the convolution operation, since the calculation size of the systolic array is relatively fixed, in order to reflect the operation efficiency of the systolic array, it is necessary to perform many formal operations on the input and weight. In addition, when doing matrix multiplication, the data needs a large number of sizes to achieve the effect of pipeline execution. For example, the calculation efficiency of a 256x256 2D systolic array on a small matrix is not high..MKN3-D MAC ArrayTPUv1NVDLA2-D MACArray[NxN](PE)NxNxNNxNABbuffer size.In addition, the related patent implements a MKN 3-D MAC Array, which further significantly improves the calculation efficiency of matrix multiplication compared to the 2-D MAC Array scheme of TPUv1 and NVDLA. This invention proposes a new hardware accelerator architecture that enables it to complete a [NxN] matrix multiplication operation in a single clock cycle. In this hardware architecture, the number of included processing units (PE) is NxNxN, and the number of included addition trees is NxN. At the same time, a calculation method for splitting large matrices into smaller matrices is also proposed. However, since the above solution needs to fill the matrix size to the size supported by the hardware, the data bandwidth is wasted and the calculation efficiency is reduced. If the matrix is artificially divided into a large matrix and a small matrix, the software programming will be complicated, and the relative software programming amount will also be greatly increased. And because the accelerator can only load the elements in the matrix in one direction, the software needs to split the matrix by itself, so the calculation mode is single and inflexible; in addition, once the memory of matrix A and matrix B cannot hold all the data, there will be repetitions. read. Therefore, the buffer size will strongly depend on the business algorithm, that is, the accelerator relies heavily on the tightly coupled on-chip memory...Therefore, the technical problem to be solved by this application is how to use hardware to perform computation on a large number of data operations in a convolutional neural network with high efficiency, flexibility and low energy consumption...It can be understood that the matrix multiplier provided by the embodiments of the present invention can be applied not only to fields such as machine learning, deep learning, and convolutional neural networks, but also to fields such as digital image processing and digital signal processing, and can also be applied to other fields. A field involving matrix multiplication operations..66660601602603604603601602604603601602604603.Based on the above analysis, the present application provides a matrix multiplication accelerator, which specifically analyzes and solves the technical problems raised in the present application. Please refer to FIG. 6. FIG. 6 is a structural diagram of a matrix multiplication accelerator provided by an embodiment of the present invention. As shown in FIG. 6, the matrix multiplier 60 includes: a first memory 601, a second memory 602, an arithmetic circuit 603, and a control The controller 604, wherein the arithmetic circuit 603, the first memory 601, the second memory 602 and the controller 604 can perform data communication through a bus. The operation circuit 603 is used to extract the matrix data in the first memory 601 and the second memory 602 and perform vector multiplication and addition operations, and the controller 604 is used to control the operation circuit 603 to complete the vector operation according to a preset program or instruction . in,.601MKaaija.ij.i(123M)j(123K).The first memory 601 is used to store the first matrix. The first matrix is an MK matrix. If the matrix a is the first matrix, the element in the i-th row and the j-th column in the first matrix a can be denoted as a.ij., i(1, 2, 3, ... ...M), j  (1, 2, 3, ... K);.601602606(randomaccess memoryRAM)int 8fp16fp32.The first memory 601 mentioned in the embodiment of the present invention, the second memory 602, the.third memory.606, and the internal memory of the correlation matrix multiplier mentioned below may all be registers, random access memory (random access memory, referred to as the internal memory for short). RAM), static random access memory, flash memory, or other readable and writable memory. In this application, the data types of the first matrix and the second matrix and the operation result may be int 8, fp16, or fp32 and other types..602KNbbjgB.jg.j(123K)g(123N).The second memory 602 is used for storing a second matrix, and the second matrix is a KN matrix. If the matrix b is the second matrix, the element of the jth row and the gth column of the second matrix b can be denoted as B.jg., j(1, 2, 3,...K), g(1, 2, 3....N);.MKNXY0MNKMNKXY.Among them, M, K and N, and X and Y are all integers greater than 0, any two parameters in M, N and K may be equal or unequal, and M, N and K may all be equal or unequal, X and Y may be equal or unequal, which is not specifically limited in this application..603XY6030(MAC)6603446031X4Y460306016026030601602.The operation circuit 603 may include X rowY column operation units 6030 (may be referred to as a multiply-accumulate unit MAC), and each operation unit may independently perform vector multiplication operations. In FIG. 6, the operation circuit 603 includes 44 The operation unit 6031 performs drawing as an example, that is, X4, Y4. The.operation unit.6030 is provided with two inputs, which are respectively used to receive the row vector and the column vector sent by the first memory 601 and the second memory 602, and perform vector multiplication operation on the row vector and the column vector. Specifically, an.operation unit.6030 includes a vector multiplication circuit and an addition circuit, wherein the matrix multiplication circuit is used for receiving row vector data sent by the first memory 601 and column vector data sent by the second memory 602, and converting the two-way vector data Multiplication; the addition circuit is used to add the results of the multiplication of the two vectors, and to accumulate the calculation results belonging to the same operation unit to obtain the calculation result of each operation unit..76030L(L4)(L1)L60605605605XY605.Referring to FIG. 7, which is a structural diagram of an.operation unit.6030, in a possible implementation manner, the vector multiplication circuit includes L (for example, L4) multipliers; the addition circuit includes an input number of ( The addition tree of L1), that is, the addition tree is used to accumulate L multiplication results and the calculation results of the operation unit in different clock cycles. Optionally, the matrix multiplier 60 further includes a.third memory.605, and the.third memory.605 is used to store the operation results of the vector multiplication circuit and the addition circuit, and the results of different clock cycles. It can be understood that, the.third memory.605 in the present application may include XY storage units, and each storage unit is used to store each operation result of the corresponding operation unit. Alternatively, each operation unit corresponds to a designated storage space in the.third memory.605 for storing each operation result..604.The controller 604 may calculate the product of the first matrix and the second matrix by performing the following actions:.604XLSRSRsrA.sr.s(123S)r(123R)60XYLXL.The controller 604 divides the first matrix into sub-blocks with a scale of XL as a unit to obtain SR sub-blocks of the same size, wherein the s-th row of the SR sub-blocks is divided into The subblocks of the r column are denoted.Asr., s(1, 2, 3,...S), r(1, 2, 3,...R). That is, for the matrix multiplier 60 in this application, as long as it is produced or shipped from the factory, the matrix data of the XY columns included in it has been fixed, and the number L of multipliers in the corresponding multiplication circuit is also Fixed, so when performing matrix operations, it is necessary to fractal the first matrix and the second matrix, that is, the matrix is divided into blocks. The division method is to divide the first matrix into blocks in units of XL sub-blocks. In the embodiment of the present invention, the purpose of the block is to split a large matrix into many small matrices that meet the size of the matrix multiplier, and then calculate the small matrices in a certain order and accumulate the values of the relevant small matrices, and finally obtain Matrix multiplication result. Not only can it be calculated flexibly, it is convenient for subsequent multiplexing and multi-level caching, which further improves computing efficiency and reduces data handling bandwidth and energy consumption..MKXLM/XK/L00.MX0(M1)(SX-M)0KY0(K1),RY-K00.It should be noted that the first matrix is an MK matrix, and there may be a situation that the first matrix cannot be divided by an integer number of sub-blocks of XL. Therefore, when M/X or K/L is not an integer, the operation can be performed by filling with 0 elements. Or, it does not participate in the calculation at the corresponding position, and assigns the result to 0. specifically,.When M%X0, the (M1)th to (SXM)th rows of the first matrix are not calculated and the result is assigned as 0; when K%Y0, the first matrix Lines (K1), to RYK are not calculated, and the result is assigned a value of 0. That is to say, on the corresponding row and column, the substantive multiplication calculation is not carried out by the operation unit, but it is treated as if it has been operated but the result is 0, so that the reading of the corresponding operation unit can be saved. and computing power..604LYRTRTrtB.rt.r(123R)t(123T)604603.Correspondingly, the controller 604 also controls the second matrix to be divided into sub-blocks with a scale of LY as a unit to obtain RT sub-blocks of the same size, wherein the RT sub-blocks are divided into The r-th row and the t-column in are denoted as B.rt., r(1, 2, 3,...R), t(1, 2, 3,...T). After the controller 604 controls the first matrix to be divided into blocks according to the specifications of the operation circuit 603, the second matrix must also be divided correspondingly to the first matrix, otherwise the matrix multiplication calculation cannot be performed..KNLYK/LN/Y00.KY0(K1)(RY-K)0NX0(N1)(TX-N)00.It should be noted that the second matrix is a KN matrix, and there may be a situation that the second matrix cannot be divided by an integer number of LY sub-blocks. Therefore, when K/L or N/Y is not an integer, the operation can be performed by filling with 0 elements. Or, it does not participate in the calculation at the corresponding position, and assigns the result to 0. specifically,.When K%Y0, the (K1)th to (RYK)th of the first matrix do not need to be calculated and the result is assigned as 0; when N%X0, the first matrix Lines (N1) to (TXN) are not evaluated and the result is assigned the.value.0. That is to say, on the corresponding row and column, the substantive multiplication calculation is not carried out by the operation unit, but it is treated as if it has been operated but the result is 0, so that the reading of the corresponding operation unit can be saved. and computing power..603604A.sr.XxB.rt.YyXYxyx(123X)y(123Y)A.sr.rB.rt.rA.sr.B.rt.A.sr.B.rt.603.After the first matrix and the second matrix are divided into blocks with fixed specifications, they can be input to the operation circuit 603 to perform matrix multiplication operation between sub-blocks and sub-blocks. In the specific calculation process, the controller 604 can control the x-th row in the X row vectors of any sub-block A.sr.in the first matrix and the y-th column in the Y column vectors of the corresponding sub-block B.rt., input into the operation unit of the xth row and the yth column of the X rowY column operation unit for operation, x(1, 2, 3,...X), y(1, 2, 3, ... Y), wherein the values of r in any one of the sub-blocks A.sr.and r in the corresponding sub-block B.rt.are equal. Because before the row vector of the sub-block A.sr.and the column vector of B.rt.are input to the operation unit, the first matrix and the second matrix have been divided into matrix blocks, that is, fractal. Therefore, in which order the sub-blocks A.sr.and the corresponding B.rt.are input into the operation circuit 603, there may be various implementations..A.sr.B.rt.st8MKKNM12K6N12X4Y4L3S3R2T3..AXL43BLY34.In a possible implementation manner, the operations may be performed successively according to the size of s in the sub-block A.sr.and the corresponding B.rt., or the size order of t. As shown in Fig. 8, for example, the first matrix is MK matrix, and the second matrix is KN, assuming M12, K6, N12; X4, Y4, L3 will After the first matrix and the second matrix are divided into blocks, S3, R2, T3. So the first matrix after the block is obtained.The second matrix after the block.And A is a matrix of XL, which is 43, and each element in B is actually a matrix of LY, which is 34..A.sr.B.rt..In the multiplication operation of the first matrix and the second matrix, a subblock matrix multiplication operation needs to be performed on any one of the first matrices, that is, each subblock A.sr.and the corresponding subblock B.rt.in the second matrix. In which order to determine which sub-block matrix multiplication calculation is performed first, various implementations may be included,.A.11.B.11.()A.11.B.11.()A.12.B.21.CC.11.CC.11.A.11.B.11.A.12.B.21..Mode 1: According to the order of matrix multiplication, for example, it can be sub-block A.11.and sub-block B.11.. In the first sub-block multiplication calculation cycle (which can be understood as the first round), input all row vectors of A.11.and corresponding B All column vectors in.11.are operated on. In the second sub-block multiplication calculation cycle (which can be understood as the second round), all row vectors of A.12.and all column vectors in corresponding B.21.are operated. In this way, after the accumulation of the operation unit, the result can be obtained. The value of the result point C.11.in the first row and first column of matrix C. By analogy, the result points of all positions on the result matrix C can be obtained. In fact, C.11 .A.11.B.11.A.12.B.21., where,.C.11.44CMN1212.That is to say, C.11.is actually a 44 matrix. Therefore, according to the calculation rules of the matrix, the finally obtained matrix C is the result matrix of MN, that is, the result matrix of 1212..A.sr.B.rt.604srtA.sr.XYA.sr.B.rt..Mode 2: One of the sub-blocks is multiplexed according to a certain rule. The embodiment of the present invention provides a sub-block multiplexing method to call a sub-block A.sr.in the first matrix and a corresponding sub-block B.rt.in the second matrix to perform sub-blocks. Block matrix multiplication operation. Specifically, the controller 604 is further configured to control: in at least two consecutive sub-block multiplication calculation cycles, the values of s and r remain unchanged, and the value of t changes, so that the first memory Multiplexing the same A.sr.in the at least two consecutive sub-block multiplication calculation cycles, wherein the sub-block multiplication calculation cycle is when the X rowY column operation unit calculates a sub-block A.sr.and a corresponding sub-block A sr The time taken for the matrix multiplication operation of the sub-block B.rt...M12K6N12X4Y4L3()A.11.B.11.()srtA.11.B.12.()A.11.B.13.A.11..For example, in the above embodiment assuming M12, K6, N12; X4, Y4, L3, in the first sub-block multiplication calculation cycle (which can be understood as the first round ) Operates on all row vectors of input A.11.and all column vectors in one of the corresponding sub-blocks B.11.. In the second sub-block multiplication calculation cycle (which can be understood as the second round), the values of s and r remain unchanged, but t needs to be changed, that is, all row vectors of A.11.and another corresponding sub-block B.12.are performed again. All column vectors of . Optionally, in the third sub-block multiplication calculation cycle (which can be understood as the third round), all row vectors in A.11.and all column vectors in another corresponding sub-block B.13.continue to be operated on. In this way, A.11.in the first storage can be reused in several adjacent sub-block multiplication calculation cycles, which saves read and write overhead and reduces data transfer bandwidth..A.sr.B.rt.A.sr.XxB.rt.YyXYxyx(123X)y(123Y)A.sr.rB.rt.rA.sr.B.rt.XYA.11.[a.21.a.22.a.23.]B.11..XY23.Wherein, in the above-mentioned.ways.1 and 2, the calculation rule for a certain sub-block A.sr.in the first matrix and the corresponding sub-block B.rt.in the second matrix in one sub-block multiplication calculation cycle is: In the first matrix The x-th row in the X row vectors of any sub-block A.sr.and the y-th column in the Y column vectors of the corresponding sub-block B.rt.are input to the x-th row in the X rowY column operation unit. The operation is performed in the operation unit of row y column, x(1, 2, 3,...X), y(1, 2, 3,...Y), wherein, in any one of the sub-blocks A.sr.The value of r is equal to the value of r in the corresponding sub-block B.rt.. That is, for A.sr.and the corresponding sub-block B.rt.in the second matrix, any row vector and any column vector thereof are input to the designated operation unit in the X rowY column operation unit for calculation. For example, the second row vector [a.21.a.22.a.23.] in A.11., and the third column vector in one of the corresponding subblocks B.11.in the second matrix.That is, the operation is performed in the operation unit corresponding to the second row and the third column in the X rowY column operation unit, and so on..660399603.Based on the arrangement of the arithmetic units in the arithmetic circuit 603 shown in FIG. 6 , see FIG. 9 , which is a schematic diagram of wiring in a specific arithmetic circuit 603 provided by an embodiment of the present invention..BUFA601BUFB602BUFC6030605603XY(X4Y4)MAC GRP R00C00MAC GRP R03C03MAC GRPXLLY.BUFA is the first memory 601 of the first matrix, BUFB is the second memory 602 of the second matrix, BUFC is the.third memory.605 that stores the calculation results of each.operation unit.6030, and the operation circuit 603 includes X rowsY columns ( Suppose X4, Y4) operation units are MAC GRP R00C00 to MAC GRP R03C03 in the figure, and each operation unit MAC GRP can enter a row vector of XL matrix and a column of LY matrix Multiplication of row vectors..6033-D MAC(MAC Cube)(Accumulator)CABCABCA/B/CA(Mbase)x(Kbase)B(Kbase)x(Nbase)C(Mbase)x(Nbase)Base603XY8816163232CABCABCMNK()MNKbase().The arithmetic circuit 603, which may be referred to as a fractal matrix multiplication unit in this embodiment of the present invention, is composed of a 3-D MAC array (MAC Cube) and an accumulator (Accumulator), and is used to execute a fractal matrix multiplication instruction, as follows: C AB, or CABC, where A/B/C is a two-dimensional matrix. The size of A is (Mbase)x(Kbase), the size of B is (Kbase)x(Nbase), and the size of C is (Mbase)x(Nbase). Base is the base size of the operation circuit 603, that is, XY, for example, 88, 1616, 3232, and so on. The above calculation operation of CAB, or CABC is called MNK matrix multiplication (and accumulation). In the actual execution process, the matrix multiplication of MNK will be fractal, and the controller will control to decompose the large matrix into the basic matrix multiplication of base size, and combine them in a specific order (the above-mentioned.method.1 or method 2) to complete..7(Base4)7MAC GroupNN(44)N(4)N1(5)()94x44x44x4.The specific architecture of the fractal matrix multiplication unit is shown in the above-mentioned Figure 7 (assuming Base4), for example, the MAC Group in Figure 7 is an NN(44) multiply-accumulator group, which consists of N(4) It consists of a multiplication unit and an N1(5) input accumulation tree. At the matrix multiplication level, the multiply-accumulator can perform a row-by-column-accumulate operation (ie, an element in the resulting matrix). There are a total of 4x4 multiply-accumulator groups in Fig. 9, and a complete 4x44x4 matrix multiplication operation can be calculated at the same time..9603A.sr.B.rt.A.sr.XB.rt.Y9BUFABUFB6030604603A.sr.B.rt.A.sr.B.rt.A.sr.B.rt..It can be understood that the schematic diagram of the wiring in FIG. 9 can support the operation circuit 603 to complete the matrix multiplication calculation of one sub-block A.sr.and the corresponding sub-block B.rt.in the same clock cycle. Because, all X row vectors of A.sr.and all Y column vectors of the corresponding sub-block B.rt.can reach the.corresponding operation unit.6030 from the corresponding BUFA and BUFB at the same time through the wiring method in FIG. 9 , therefore, The controller 604 can control the arithmetic circuit 603 to complete the multiplication calculation of one sub-block A.sr.and the corresponding sub-block B.rt.in one clock cycle, and complete another sub-block A.sr.and the corresponding sub-block B.rt.in the next clock cycle..,., or, a matrix multiplication calculation of the same sub-block A.sr.and another corresponding sub-block B.rt...101060310603604A.sr.xXYx1B.rt.yXYy1.Referring to FIG. 10, FIG. 10 is a schematic diagram of wiring in another specific operation circuit 603 provided by an embodiment of the present invention. In the operation circuit 603 corresponding to FIG. 10 , a systolic array structure is provided. Specifically, the controller 604 is configured to control the row vector of any A.sr.to enter the xth row corresponding to the X rowY column operation unit in the order of the x row numbers from small to large, and the adjacent row vectors enter The time difference to the operation units of the same column and different rows is 1 clock cycle; the controller is also used to simultaneously control the column vector of the corresponding sub-block B.rt.to enter into the The yth row corresponding to the X rowY column operation unit, and the time difference between adjacent column vectors entering the operation units of the same row and different columns is 1 clock cycle..6030()TPUv1L(TPUv11)TPUv1.That is, in order to make full use of each operation unit 6030 (multiply-accumulator), the fractal matrix multiplication unit in the embodiment of the present invention may be a systolic array structure, which is different from the structure of TPUv1 in that the amount of data transmitted in each systolic is L (and 1 in TPUv1), therefore, the parallelism of data operations is greater than the systolic array in TPUv1..10BUFA/B/10(BUFA)AXL(BUFB)YL.Based on the systolic array architecture, in the above-mentioned wiring structures corresponding to FIG. 10 , BUFA/B are the memories used to cache the first matrix/second matrix, respectively. The identity matrix of is divided into X rows and the L elements of the same row are sequentially fed into an arithmetic unit in the systolic array at each clock cycle. Likewise, the second matrix buffer device (BUFB) divides the identity matrix in the second matrix into Y columns and sequentially feeds L elements of the same column into the systolic array every clock cycle. The specific timing is as follows:.BUFCABCC()(L0)BUFCLBUFC1.BUFC is a cache device that stores the \"C\" (offset) matrix in the \"ABC\" calculation (can be built by L0 cache or cache register), and the intermediate value in matrix multiplication can also be stored with BUFC. When the multiply-accumulator completes the multiplication, the accumulation tree will accumulate the multiplied L intermediate values and an offset or intermediate value stored in the BUFC..M2N2K2(8x88x8)606031184x4MNKMNK6038121512T0M2N2K213T1M2N2K214T7M2N2K215T11M2N2K2T676.Taking M2N2K2 (that is, 8x88x8 matrix multiplication) as an example, the controller 603 in the matrix multiplier 60 will split the matrix multiplication into the format of FIG. 11 , a total of 8 4x4 unit matrix operations . For the matrix multiplication operation of MNK, there are many possibilities for the order of splitting, and the rules can be calculated in the order of the.above method.1 and.method.2. It is understandable that using the strategy of maximum data reuse in.method.2 can reduce the number of data reads Take the power consumption. After the MNK fractal splitting is completed, the control logic of the controller 603 will divide the eight fractals into the systolic array in eight clock cycles, as shown in FIG. 12 to FIG. 15 . Among them, Figure 12 is the pipeline execution of the fractal matrix multiplier with M2N2K2 at the time of T0, Figure 13 is the pipeline execution of the M2N2K2 matrix multiplier at the time of T1, and Figure 14 For the pipeline execution of the M2N2K2 fractal matrix multiplier at the time of T7, FIG. 15 shows the pipelined execution of the M2N2K2 fractal matrix multiplier at the time of T11. It can be seen that when T6, that is, the seventh clock cycle, the systolic array starts to run at full load. In the last 6 clock cycles, the identity matrix fractal sends out the systolic array, and the entire matrix also completes the multiplication operation..166060660760860961061160(Central Processing UnitCPU)80CPU60CPU8070607070(Double Data Rate Synchronous Dynamic Random Access MemoryDDR)6060160260570(On-Chip Buffer),.Optionally, referring to FIG. 16 , the matrix multiplier 60 may further include an.instruction dispatching unit.606 , an.instruction fetching unit.607 , a data handling unit 608 , a vector unit 609 , a scalar unit 610 , and a bus interface unit 611 . Further, the matrix multiplier 60 provided in the embodiment of the present invention may be mounted on a central processing unit (Central Processing Unit, CPU for short) 80 as a coprocessor, and the CPU allocates computing tasks to the matrix multiplier 60. Specifically, the.CPU.80 The first and second matrices and related instructions may be stored in the external memory 70 , and the matrix multiplier 60 may perform matrix multiplication operations by reading the first and second matrices and related instructions in the external memory 70 . The external memory 70 may specifically be a double data rate synchronous dynamic random access memory (Double Data Rate Synchronous Dynamic Random Access Memory, DDR for short) or other readable and writable memory, and the external memory may be a memory private to the matrix multiplier 60 . Specifically, the first memory 601, the second memory 602, the.third memory.605 and the external memory 70 are generally on-chip memories (On-Chip Buffer), wherein.1609(Vector Unit)()SIMD(Single Instruction multiple data)(Unified Buffer)L0C().1. Vector Unit 609 (Vector Unit): Contains various types of multi-parallel computing devices (such as floating-point multiplication, floating-point addition, floating-point size comparison, etc.), which are used to execute SIMD (Single Instruction multiple data) instructions. It is also responsible for the direct data transfer between the Unified Buffer and the LOC cache (ie, the first memory, the second memory, and the third memory)..2610(Scalar Unit)().2. Scalar unit 610 (Scalar Unit): contains various types of basic arithmetic devices for shaping (such as addition, multiplication, comparison, shift, etc.)..3(Direct Memory Access UnitDMA Unit)L1RAML0RAM22.A0A1A2A3.3. The data handling unit (Direct Memory Access Unit, DMA Unit) is used to handle the data in each storage unit, such as data from L1RAM to LORAM, the data handling unit in the embodiment of the present invention is in the slave matrix multiplier. When the external memory or internal memory is used to transport the matrix data involved in the multiplication operation, the matrix needs to be stored according to the result of the block. For example, for a 22 matrix, the first row of the first matrix subblocks of the first column.If it is stored in blocks, A0, A1, A2, and A3 are stored as a row, and so on. In this way, when the first matrix or the second matrix is transferred to the corresponding first memory, or the second matrix is transferred to the corresponding second memory, it can be stored in the above-mentioned manner, and when the operation unit needs to read , can also be read according to the above storage order, so that when a row vector needs to be transposed into a column vector during calculation, it can be transposed flexibly and quickly..4607(Instruction Fetch UnitIFU)PC()IM()(BIU)611.4. The instruction fetch unit 607 (Instruction Fetch Unit, IFU), that is, the instruction fetch unit, integrates PC (program counter) and IM (instruction memory) internally, and obtains instructions from the main memory through the bus interface unit (BIU) 611 and decodes them and control the execution flow..5606(Dispatch Unit)416(Scalar Unit)(Direct Memory Access UnitDMAUnit)(VectorUnit)4.5. The instruction dispatch unit 606 (Dispatch Unit) parses the instruction transmitted by the instruction fetch unit, and submits the corresponding type of instruction to the four pipeline units, wherein the pipeline unit is the scalar unit (Scalar) in FIG. 16 . Unit), data transfer (Direct Memory Access Unit, DMAUnit) unit, vector unit (VectorUnit), fractal matrix multiplication unit. The instruction distribution unit has a mechanism to control the execution order between the four pipelines..(Posted Execution)(Scalar Unit)(Fractal MatMult Unit)DMA(Vector Unit).It should be noted that, the above-mentioned pipeline unit has two types: asynchronous execution (Posted Execution) and synchronous execution. All types of instructions are issued in order, the difference is that the asynchronous execution unit executes the instruction asynchronously, and the synchronous execution unit ends synchronously; the scalar unit (Scalar Unit) is synchronously executed; the fractal matrix multiplication unit (Fractal MatMult Unit), DMA unit and Vector Units are executed asynchronously..()().In a possible implementation manner, for the above-mentioned data handling unit, an embodiment of the present invention provides a configurable path-associated matrix transposition function. For example, when one of the block matrices of the first matrix is transferred from a certain memory (such as the external memory of the matrix multiplier) to another memory (the internal memory of the matrix multiplier, the first memory), the data transfer unit will be in the middle of the transfer. Perform the operation of transposing the matrix and store it in the order of the transposed matrix. Because matrix transposition is a necessary operation link in the neural network training process. Compared with the common instruction of first handling and then transposing, the handling instruction that can be configured to be transposed with the path matrix in the embodiment of the present invention is more flexible, and also makes the software easier and more concise. As shown in the table below,..Ordinary instructions: Configurable instructions for the matrix transpose function with the path:..Comparing the common handling instruction with the handling instruction with the configurable path-dependent matrix transpose function, by supporting the configurable path-dependent matrix transposition function, the same instruction can support more application scenarios with different parameters. A configurable path-dependent matrix transpose method for fractal matrix multiplication processor architecture is designed..17(Unified Buffer)L1L0L1DMAL0L1L2L0MNK,,D(DFF)L1(17612613)L0(17601602)L1.Referring to FIG. 17 , in order to facilitate data reuse, reduce power consumption, and reduce dependence on tightly coupled on-chip memories, an embodiment of the present invention further provides a storage structure using multi-level caches. All computing units can read and write interactive data through the Unified Buffer. There are L1 and L0 two-level dedicated caches inside the matrix multiplier. The L1 cache and the unified cache exchange data through the data transfer DMA unit and the external storage space; the external storage space is composed of multi-level storage units. For example, the matrix multiplier contains multi-level cache, from L0, to L1, and then to L2 cache, the capacity increases, the access bandwidth decreases, the delay increases, and the power consumption increases. L0 is the innermost cache, which can be used to cache the three matrices of \"first matrix\", \"second matrix\" and \"result matrix\" in the MNK multiplication instruction. Because it is close to computing, it has the highest requirements on bandwidth and delay, and has the greatest opportunity for data reuse. It can be built with D flip-flops (DFF) to improve performance and reduce power consumption. The source and destination operands of the fractal instruction come from L1 (the.fifth memory.612 and the.fourth memory.613 in FIG. 17 ). Data is multiplexed by means of L0 (ie, the first memory 601 and the second memory 602 in FIG. 17 ) during execution. Software above fractal instructions can reuse data with the help of L1. Using the execution sequence of the fractal instruction and the software control sequence above the fractal instruction, the multi-level cache data reuse can be realized. At the same time, with the reuse of multi-level cache data, the data transfer time of data in each cache can also be masked. The following chart example can illustrate data multiplexing and moving between multiple levels of caches:...Suppose you have the following two matrices:.The data transfer steps are shown in the following table:..time.L1.read L1.L0.Deposit to L0..calculate.1.1.A0B0.A0,.B0.2.2.B1.B1.A0B0.A0, B0.A0B0.A0.B0.3.3.A2.A2.AOB0B1.AO, B0, B1.A0B1.A0.B1.4.4.A1.A1.A0A2B0B1.A0, A2, B0, B1.A2B0.A2.B0.5.5.B2.B2.A1A2B0B1.A1, A2, B0, B1.A2B1.A2.B1.6.6.B3.B3.A1A2B1B2.A1, A2, B1, B2.A1B2.A1.B2.7.7.A3.A3.A1A2B2B3.A1, A2, B2, B3.A1B3.A1B3.8.8.A2A3B2B3.A2, A3, B2, B3.A3B2.A3B2.9.9.A2A3B2B3.A2, A3, B2, B3.A3B3.A3B3.160L1A0B0L0.At.time.1, the controller 60 reads the A0, B0 portion of the matrix from the L1 cache and stores it in L0..2A0B0L0L1B1L0B13A0B11A0.At.time.2, the A0 and B0 fractal matrices can already be read from L0 and participate in the operation. At this time, the hardware will read the B1 fractal from L1 and store it in L0 to prepare for the next operation. At the same time, the time to read the data is are also masked by calculations. At this time, the hardware does not need to read the two fractal matrices at the same time, but only reads the B1 matrix. Because the matrix calculation \"A0B1\" at.time.3 multiplexes the data A0 stored at.time.1. Referring to the above list, it can be seen that in the subsequent calculation, there will be data multiplexing for each time unit..L1L0L2(701702)L1.It should be noted that the embodiment of the present invention is not limited to data transfer between L1 and L0, and data transfer from L2 (for example, external memory 701 and external memory 702 ) to L1 cache can also use data multiplexing to reduce bandwidth , the purpose of optimizing energy consumption. The embodiments of the present invention do not limit the manner of splitting matrix data and the order of handling, and data handling should maximize data multiplexing, so that the fractal matrix calculation runs at full load in each time unit...In the embodiment of the present invention, through the multi-level cache structure, the data reuse of the matrix fractal, the execution order of the fractal instructions, and the software control order above the fractal instruction can be used to realize the multi-level cache data reuse and reduce the dependence on the tightly coupled on-chip memory. Optimized energy efficiency and reduced software programming complexity...In the embodiment of the present invention, the execution sequence of the instruction for performing the multiplication operation of the matrix includes two modes: synchronous execution and asynchronous execution:.(commit).Because in this embodiment of the present invention, a series of control preparations and data preparations are required before the fractal matrix multiplication instruction is executed, such as: calculation of matrix size, reading of matrix data, calculation of target address, and the like. If the processor's instruction execution strategy is synchronous execution, that is, all instructions need to be returned in order (commit), the instructions are likely to wait for the completion of unrelated instructions before starting to execute. This will cause a large and unnecessary performance loss, as follows: Synchronize the execution order of the instructions..0011.Address calculation  control preparation .matrix.0 read .matrix.0 multiplication  address calculation  control preparation .matrix.1 read .matrix.1 multiplication..10606(WaitFlag)18.In the above execution sequence, the control preparation, address calculation, and data reading of.matrix.1 for the second time do not depend on the end of the multiplication of.matrix.0, and the extra time will cause unnecessary waiting time. In order to solve this problem, in the embodiment of the present invention, the hardware.instruction dispatching unit.606 adopts multi-channel order-preserving transmission, so as to ensure that different types of instructions can be sequentially executed simultaneously. As in the above example, the control preparation and address calculation are executed in the scalar channel, the matrix reading and storage are executed in the data handling channel, and the matrix multiplication calculation is also executed in the matrix operation channel. Each channel can overlap each other without preserving the order, and interdependent instructions can be synchronized by setting the wait flag (WaitFlag). Through the strategy of asynchronous execution of instructions, parallelism can be achieved between instructions, which improves the operation efficiency. If the above example of the synchronous execution sequence adopts the asynchronous execution strategy, the effect is shown in Figure 18. In the asynchronous execution sequence of instructions, the order of each instruction is not preserved, and the related instructions with dependencies can be synchronized by the waiting instructions added by the software. The control preparation overhead of fractal matrix multiplication can be masked by this asynchronous execution. An asynchronous execution method suitable for the programming method of fractal matrix multiplication is designed..MNK60604(XL x LY)604603XYFC.A matrix multiplier is provided, and the matrix multiplier utilizes the controller to complete a block method of matrix multiplication, namely MNK fractal, and divides the large matrix by the control logic of the internal controller 604 in the matrix multiplier 60. Multiplies the identity matrix (i.e. the matrix of XL x LY). The control logic of the controller 604 sends the unit matrix multiplication task to the operation circuit 603 every clock cycle, so that the data pipeline is executed, so that the X rowY column operation unit operates at full load. The efficiency of matrix multiplication is improved, and the application effect of neural network algorithm is significantly improved. The matrix multiplier provided by the embodiment of the present invention can perform the convolution operation and the FC operation in the convolutional neural network..((Digital Subscriber LineDSL))()()(DVD)((Solid State DiskSSD)).In the above-mentioned embodiments, it may be implemented in whole or in part by software, hardware, firmware or any combination thereof. When implemented using a software program, it may be implemented in whole or in part in the form of a computer program product. The computer program product includes one or more computer instructions. When the computer program instructions are loaded and executed on a computer, the processes or functions described in the embodiments of the present application are generated in whole or in part. The computer may be a general purpose computer, special purpose computer, computer network, or other programmable device. The computer instructions may be stored in or transmitted from one computer-readable storage medium to another computer-readable storage medium, for example, the computer instructions may be downloaded from a website site, computer, server, or data center The transmission is carried out to another website site, computer, server or data center by wire (eg coaxial cable, optical fiber, Digital Subscriber Line, DSL for short) or wireless (eg infrared, wireless, microwave, etc.). The computer-readable storage medium can be any available medium that can be accessed by a computer or data storage devices including one or more servers, data centers, etc. that can be integrated with the medium. The usable media may be magnetic media (eg, floppy disks, hard disks, magnetic tapes), optical media (eg, DVD), or semiconductor media (eg, Solid State Disk (SSD) for short), and the like..(comprising).Although the present application is described herein in conjunction with various embodiments, those skilled in the art will understand and understand, by reviewing the drawings, disclosure, and appended claims, in practicing the claimed application. Other variations of the disclosed embodiments are implemented. In the claims, the word \"comprising\" does not exclude other components or steps, and \"a\" or \"an\" does not exclude a plurality. A single processor or other unit may fulfill the functions of several items recited in the claims. The mere fact that certain measures are recited in mutually different dependent claims does not indicate that these measures cannot be combined to advantage...Although the application has been described in conjunction with specific features and embodiments thereof, it will be apparent that various modifications and combinations can be made therein without departing from the spirit and scope of the application. Accordingly, this specification and drawings are merely exemplary illustrations of the application as defined by the appended claims, and are deemed to cover any and all modifications, variations, combinations or equivalents within the scope of this application. Obviously, those skilled in the art can make various changes and modifications to the present application without departing from the spirit and scope of the present application. Thus, if these modifications and variations of the present application fall within the scope of the claims of the present application and their equivalents, the present application is also intended to include these modifications and variations."
}