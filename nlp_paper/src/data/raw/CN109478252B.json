{
  "patent_id": "CN109478252B",
  "title": "Multicast network and memory transfer optimization for neural network hardware acceleration",
  "assignee": "Intel Corp",
  "description": "Description.Translated from.Chinese..Multicast Network and Memory Transfer Optimization for Neural Network Hardware Acceleration..Cross References to Related Applications.20165762/333214Memory andProcessing Architecture for Hardware Accelerated Machine Learning.This patent application claims priority to Provisional Patent Application Serial No. 62/333214, filed May 7, 2016, entitled \"Memory and Processing Architecture for Hardware Accelerated Machine Learning,\" which is hereby incorporated by reference in its entirety...Background technique.AppleSiri.TM.Microsoft Cortana.TM.Amazon Alexa.TM.Google Assistant.TM..Machine learning and deep neural networks, including deep trust networks (collectively referred to as neural networks), are rapidly becoming popular. Applications started with object recognition in computer images and from speech recognition, now common in voice user interfaces such as Apple Siri.., Microsoft Cortana.., Amazon Alexa.., Google Assistant... Neural networks are currently being used in industrial controllers, medical diagnostics, which has led to the rapid growth of neural networks..CPU.However, neural network operations, at least when applied to machine learning and deep neural networks, typically utilize dense linear algebra operations such as matrix operations, as well as more neural network-specific operations such as convolution, max pooling, and data noise generation. Such operations facilitate parallel operations, such as computing matrix rows in parallel, which would result in sub-optimal performance if performed on common central processing units (CPUs) which are generally not parallel..GPUNVidiaCUDA.TM.GPU.Accordingly, arrays of graphics processing units (GPUs) optimized for matrix operations and parallel operations have been employed for neural networks, such as via NVidia's CUDA..architecture. However, while GPUs are optimized for matrix operations, they do not provide optimizations specific to neural networks (such as convolution, max-pooling, and noise generation), thereby limiting their performance in neural network operations...Description of drawings..The detailed description is provided with reference to the accompanying drawings..1.Figure 1 is a scenario diagram of a system environment for machine learning hardware acceleration..2.Figure 2 is a block diagram for machine learning hardware acceleration..3.Figure 3 is a block diagram optimized for multicast networks for machine learning hardware acceleration..4.Figure 4 is a flowchart for multicast network optimization for machine learning hardware acceleration..5.FIG. 5 is a scenario diagram of access spans for non-interruptible volumetric computer memory..6DRAMpermutaton.Figure 6 is a block diagram of a permutaton used in DRAM transfer optimization for machine learning hardware acceleration..7DRAM.Figure 7 is a block diagram optimized for DRAM transfers for machine learning hardware acceleration..8DRAM.Figure 8 is a flowchart optimized for DRAM transfers for machine learning hardware acceleration...Detailed ways..An overview of multicast network and memory transfer optimization for neural network hardware acceleration./.Neural network hardware acceleration occurs within the context of the environment in which applications that utilize neural networks are developed, compiled (or programmatically upconverted), and executed. Such applications are often referred to as machine learning applications, deep neural network applications, and/or deep trust network applications. While machine learning does not strictly require the use of neural networks, many common today's frameworks and techniques make use of neural networks. A deep neural network can be roughly thought of as a series of neural networks or a network of neural networks..CPUGPU.As mentioned above, today's hardware in the form of central processing unit (CPU) or graphics processing unit (GPU) arrays does not provide hardware optimizations for many operations common to neural networks. Disclosed herein are various techniques for neural network hardware acceleration, in particular for multicast networks for data dispatched to data sinks, such as execution units, and for memory transfers..Big O(C)Big O(n)CPU/GPUBig O(n log(n))/.The optimizations disclosed herein are designed to perform those operations in hardware in constant time (Big O(C)) or linear time (Big O(n)), otherwise the CPU and/or GPU would use Big O(n log(n)) or higher polynomial time. Optimizations can leverage information at design time and/or compile time, can leverage transformations to implement multidimensional operations common to matrices as well as tensor operations, and can identify and exploit instruction pipelining opportunities in hardware..1100102104.FIG. 1 provides a scene graph 100 in which neural network hardware acceleration can occur. Specifically, users 102 access computing services from cloud 104 . Users can be developers or can be end users..104106108106110.Cloud 104 includes number of servers 106 capable of storing computer-readable data and executing computer-readable instructions. Those servers 106 may be broken down by a manager 108 to serve virtual machines 110 ..112106110/114106/110116106106108110.Compiled neural network application 112 may execute directly on server 106 or on virtual machine 110 . Server 106 and/or virtual machine 110 may be provisioned by one or more neural network frameworks and/or runtime 114 . The neural network hardware acceleration unit 116 may be connected to the server 106 or may be stand-alone. As a resource of the server 106 , the neural network hardware acceleration unit may also be decomposed by the hypervisor 108 to make its resources available to the virtual machine 110 ..112118120112114.The compiled neural network application 112 is the result of compiling the source code 118 for the neural network application by the compiler 120 . The neural network application 112 may also have been linked to libraries specific to the neural network framework or runtime 114 ..116122124CPU128130132126132134136.Returning to the neural network hardware accelerator unit 116, it includes a system control block 122 that can branch instructions and perform other operations. It interfaces with the control CPU via a communication bus 124 . The hardware accelerator unit will have an instruction interpreter 126 that interfaces with local memory 128 , one or more multicast networks 130 , and a plurality of data receivers 132 . In some embodiments, data receiver 132 may be an execution unit. The interface to data off-device may be interfaced on memory bus 136 via data transfer unit 134 ..2116130134345678.The neural network hardware accelerator unit 116 is described in further detail below with respect to FIG. 2 . Note that the one or more multicast networks 130 and the data transfer unit 134 have several optimizations. Multicast network optimization is described in further detail below with respect to FIGS. 3 and 4 . The data transfer unit optimization exploits features of group theory as described below with respect to FIG. 5 . The data transfer unit optimization itself is described below with respect to FIGS. 6 , 7 and 8 ...Exemplary Architecture of a Neural Network Hardware Acceleration Unit.11621200.A closer examination of the neural network hardware acceleration unit 116 is worthwhile. FIG. 2 provides a block diagram 200 of a neural network hardware acceleration unit augmented with details described with respect to FIG. 1 ..202206204CPUPCIPCI-E.A neural network hardware accelerator unit 202 , which may interface with a server or with some other controlling CPU via a parallel bus or serial bus 206 via a system control block 204 . In some implementations, the interface is a PCI bus or a PCI-E bus. However, any standardized bus will suffice. A serial bus can be used, but at the performance cost of serialization overhead../208210204208SRAMSRAM208(a)208(b).Computer instructions and/or opcodes may be stored in local memory 208 and interpreted by instruction interpreter 210 . Computer instructions may be reached via system control block 204 . Local memory 208 may be static random access memory (SRAM). SRAM may be subdivided into locations for computer instruction interpretation and execution, and one or more regions 208(a), 208(b) of working memory, where each region may be subdivided in at least some portion into banks of memory..208(a)208(b)212(a)212(b)208(a)208(b)214.At least some of the areas of working memory 208(a), 208(b) may each be associated with a multicast network 212(a), 212(b) comprised of switching nodes that dispatch data stored in the working memory areas 208(a), 208(b) to one or more data receivers 214..34212(a)212(b)208(a)208(b)214214212(a)212(b).As described in further detail with respect to FIGS. 3 and 4 , the switching nodes making up the multicast networks 212 ( a ), 212 ( b ) are organized into layers, with the first layer next to the memories 208 ( a ), 208 ( b ) and the last layer next to the data receivers 214 . In some permutations of connections, the switching node including the last layer accesses the data sink 214 . Note that the connection permutations of the last layers of the different multicast networks 212(a), 212(b) need not each be identical..214112214214.Depending on the application, data receiver 214 may be one of several embodiments. For neural network application 112, data receiver 214 may be a plurality of execution units 214 each capable of executing computer-executable instructions..208216218DRAM216DRAM218DRAM.Data may be transferred from local memory 208 to off-board memory, which may be performed by data transfer unit 216 over data bus 218 . Where the off-board memory takes the form of dynamic random access memory (DRAM), data transfer unit 216 is a DRAM transfer unit and data bus 218 is a DRAM bus...Multicast Network Optimization for Neural Network Hardware Acceleration.212(a)212(b)208(a)208(b)208(a)208(b)212(a)212(b)Bene/.The multicast network 212(a), 212(b) is designed to rearrange and replicate data from the memory 208(a), 208(b) in order to deterministically feed portions and permutations of the data in the memory 208(a), 208(b). To achieve this, the multicast network 212(a), 212(b) is configured as a Bene network, which is a collection of switching nodes organized into layers, where each switching node in a layer is capable of replicating and/or forwarding data to one or more switching nodes in a subsequent layer. When the input data has passed through all layers, the data will have been rearranged into the desired permutation..Bene214Bene.This feature of Bene networks is expected for neural network operations utilizing multidimensional matrices called tensors. Tensors may be stored in uninterrupted memory, meaning that each of the data elements making up the tensor resides in a block of memory with ordered and uninterrupted memory addresses. Multicasting the data elements to the data receiver execution unit 214 enables parallel operations on those multicast data elements through a Bene network capable of selecting and permuting arbitrary data elements..BeneBig O(c)3302(a)302(b)300304.For the purpose of hardware acceleration, configuration and operation can be reduced to constant time (Big O(c)) by making Bene network configuration data globally accessible to all switching nodes in a multicast network, and by pipelined execution instructions. FIG. 3 is a block diagram 300 of two multicast networks 302 ( a ), 302 ( b ) that transpose incoming data into data receivers 304 ..302(a)302(b)306(a)306(b)5.Each multicast network 302(a), 302(b) receives input data, typically in the form of tensors of data elements, from an area of working memory 306(a), 306(b) organized into banks. As will be seen with respect to Figure 5, organizing the data elements of a tensor into banks facilitates further optimization..302(a)302(b)310308306(a)306(b)304.Each multicast network 302(a), 302(b) is comprised of switching nodes 308 organized into layers 310. The tiers are arranged with the first tier next to the memory 306(a), 306(b) and the last tier next to the data sink 304..302(a)302(b)302(a)302(b)302(a)304308302(a)302(b)304308302(b)302(a)302(b).One purpose of implementing two multicast networks 302(a), 302(b) is that in tensor operations, it may be desirable to access different partitions of a tensor. For example, in a two-dimensional tensor (matrix), a first multicast network 302(a) may perform operations on rows, while another multicast network 302(b) may perform operations on columns. For this reason, the permutation of the switch node 308 from one multicast network 302(a) that interfaces with the data receiver 304 (i.e., the last layer of switch nodes of the first multicast network 302(a)) need not be the same permutation of the switch node 308 that interfaces with the data receiver 304 (i.e., the last layer of switch nodes of the second multicast network 302(b)) from another multicast network 302(b). In one embodiment, the first multicast network 302(a) permutation is modulo and the second multicast 302(b) permutation is grouped fanout..308306(a)306(b)310308314314316.An individual switch node 308 may contain one or more data entries received from memory 306(a), 306(b), or from a switch node 308 from a previous stage 310. A switch node may contain a configuration indicator 314 as well as a controller indicator. Configuration indicator 314 specifies whether broadcast mode is to be implemented, where incoming data is to be forwarded according to configuration data, or traffic mode is to be implemented, where incoming data is forwarded regardless of configuration data. Controller indicator 316 specifies whether at least one switch node entry is to be updated..318308308316314318308316314318.There may be a separate global configuration data store 318, either in the form of registers or in the form of memory. The global configuration data is accessible to all switch nodes 308 and holds the values of the controller indicator 316 and the configuration indicator 314 of the switch nodes 308, respectively. Because configuration data store 318 is globally accessible, in some embodiments it is possible that switch node 308 does not have locally stored values for controller indicator 316 and configuration indicator 314 , but may only have access to global configuration data store 318 ..4302(a)302(b)400.FIG. 4 is a flowchart 400 of possible operations of a multicast network 302(a), 302(b)..402302(a)302(b)306(a)306(b)302(a)302(b)308404318.Block 402 begins configuration of the multicast network 302(a), 302(b) by retrieving configuration data retrieved from a known address in computer memory 306(a), 306(b). The retrieved configuration data is used to configure the switching nodes 308 that make up the multicast networks 302(a), 302(b). The retrieved configuration data is then stored in the global configuration data store 318 in block 404 ..406306(a)306(b)312.In block 406, the data elements in the computer memory 306(a), 306(b) to be manipulated may be stored in the data entry storage 312 of the switching node..308318408302(a)302(b)308310314316318.Since all switching nodes 308 have access to the global configuration data store 318, in block 408 at least the first tier 310 of switching nodes 308 in the multicast network 302(a), 302(b) may have their respective configuration indicators 314 and controller indicators 316 populated with the control data in the global configuration data store 318..302(a)302(b)402404408302(a)302(b)Big O(c).Note that at this point, multicast networks 302(a), 302(b) are configured. If blocks 402 and 404 are executed in one clock cycle, and block 408 is executed in one clock cycle, then in effect (disregarding the insertion of no-op instructions (also called no-ops)), the multicast network 302(a), 302(b) is configured in two clock cycles, regardless of the amount of data. In fact, multicast network configuration is implemented in constant time (Big O(c))..408Bene308314308316312304.In block 408, the Bene multicast operation at the first level of switching nodes starts rearranging and copying the data elements stored in those switching nodes. The switch node 308 will determine whether to use the configuration information, or let the data pass regardless of the configuration based on the configuration indicator 314 . The switch node 308 also considers the controller indicator 316 to determine which mode to use to permute the data entry 312 to the next tier of the switch node (or in the case of the last tier, to the data receiver 304)..410308310Bene412Bene304.In block 410 , each layer 310 through the switch node 308 sequentially performs the Bene multicast operation until, in block 412 , the last layer performs the final Bene multicast operation to permute the data element into the data receiver 304 ..316314.Note that, in the general case, since operations proceed regardless of the values of controller indicator 316 and configuration indicator 314, operations can occur in one clock cycle, skipping the operations of loading and propagating control information...A background introduction to group theory for memory transfer optimization.tiles.Before discussing memory transfer optimization, background in the group theory support disclosed herein is in order. Common operations in neural networks include tensor operations, involving one partition of a tensor being operated on another partition of that tensor or a different tensor. Partitions consist of data elements that are regularly separated within their tensors. The data elements that make up a partition may be referred to as tiles...Because the tiles that make up the partition can operate independently on their operands, this creates an opportunity to perform operations in parallel, saving processing time considerably. Therefore, it is useful to have the ability to retrieve and move tiles of partitions of tensors with as few operations as possible...Group theory is the branch of mathematics that describes sets and their corresponding behavior on operators. For example, the set of integers is the group for addition, since the addition of any two integers yields an integer. There are other aspects of generating collections of groups..DDDC.D.C.D.D55025005040D-1506508.A group is a finite group of integers modulo D, where D is some positive integer. Such a group is also called a cyclic group D, denoted herein as C.D.. The memory technology herein utilizes a cyclic group C.D., where D is the number of banks in the working group of the memory. FIG. 5 is a schematic diagram 500 of such a working area 502 . Memory bank 504 is indexed from 0 to D1 and stores a number of data elements 506 . The data elements making up partition 508 are indicated in parentheses..DOO(Mi)%DODM.Data elements are stored in non-stop memory. Note that uninterrupted means that data elements are stored in consecutive, uninterrupted memory addresses. A memory address need not be a physical address, but can also refer to a virtual memory space. Because the partitions are separated at regular intervals, and because we access data elements across those regularly spaced distances (called \"strides\") (i.e., every Dth element plus some offset O), we can identify the bank storing the desired data element according to the formula O(Mi)%D, where O is the starting offset of the memory storing the tensor, D is the number of banks, and M is the stride of the tensor in memory. This capability allows us to access tiles in tensors potentially within a single processor instruction with a constant number of operations..DMDDMD.In order to avoid conflicts, the number D of memory banks should be a prime number, and the span M of data elements is not an integral multiple of D. Alternatively, the number D of banks should be relatively prime to the stride M of data elements, and the partitions to be retrieved should be vectors with fewer than D data elements..M5O2D775757.Let's say we want to access every fifth element. This means that M5. Let us also assume that the start offset address O is 2, and the number D of banks is seven. Thus, we can read up to 7 elements, each of which is read from a different bank of memory, since 5 and 7 are relatively prime. (Most certainly, 5 and 7 are both prime numbers in their own right.) Therefore:.OMMDiOMOMD1D-1DD-1MDDD-1D-1DM.For a given value of O and M, as long as M and D are relatively prime, we can always permute logically arranged data elements such that each access i goes to a unique bank. However, in a hardware implementation, for arbitrary O and M, we must physically perform this permutation. In order to remove the influence of O, it is sufficient to perform a rotation. A fast hardware implementation of rotation is a matter of understanding. To deal with the effect of the span M, we rely on another property of prime fields. Specifically, for a prime number D, the multiplicative modulus D of the elements 1 to D-1 also form a group. In fact, this group is isomorphic to the cyclic group D-1. If M is not an integral multiple of D, this means that we can achieve the effect of multiplying by M by first applying a fixed permutation to the mapping from the multiplicative group of D to the cyclic group D-1, followed by a rotation in the group D-1, followed by another fixed permutation to get back to the multiplicative group of D. The hardware to achieve fixed permutation can be done by wiring in metal layers, and rotation as mentioned earlier is also well understood..D-1r.Now we need to determine the fixed permutation usage and calculate the amount of rotation within the cyclic group D-1, which we will call 'r'..D73.In order to define these permutations, we have to choose generators over the multiplicative prime field in question. A generator of a group is an element that, through repeated application, yields all the elements of the group. For example, for a prime field based on D7, 3 is the multiplicative generator:.3.1.(mod 7)  3.3.1.(mod 7)  3.3.2.(mod 7)  2.3.2.(mod 7)  2.3.3.(mod 7)  6.3.3.(mod 7)  6.3.4.(mod 7)  4.3.4.(mod 7)  4.3.5.(mod 7)  5.3.5.(mod 7)  5.3.6.(mod 7)  1.3.6.(mod 7)  1.gDgglog.g.(x)yg.y.(mod D)xg3D7log.g.(6)3.The selected generator is denoted g. Note that D and g are fixed at design time. The discrete logarithm to base g log.g.(x) can be defined for a value y such that g.y.(mod D)x. For example, for g3, D7, we calculate log.g.(6)3..Mmlog.g.(m).Since the rotation to handle the multiplicative part of the permutation takes place in cyclic space, discrete logarithms need to be computed to determine the amount to rotate, which is complex to perform in hardware. In a practical implementation, it may be assumed that M and thus m and log.g.(m) are known a priori. This allows the compiler to perform the appropriate calculations and provide a fixed constant for the rotation. Specifically, to determine the necessary rotation, we compute:.n(log.g.(m)l)%(D-1).n(log.g.(m)l)%(D-1).oriDb(OMi)%Don.That is, to specify a transformation, o and r are provided when permuting. Note that the permutation network can be configured in one of two different variants. The first variant is called a forward modulo permutation network, which maps each iD to the corresponding appropriate bank location b(OMi)%D (given the correct o and n). This permutation is used to send addresses and data to memory banks (ie, for memory write operations). The second variant is called a reverse modulo permutation network, which simply performs the inverse mapping of the forward modulo permutation network and is used to properly rearrange the read data elements due to memory read operations..maptocylicD-11iDD-1ilog.g.(i)mapfromcylicig.i.(mod D)006.Before being able to describe forward and reverse modulo-permutation networks in more detail, two simple wiring patterns are defined. Two routing patterns are used to implement mapped logic groups and cyclic groups. Specifically, a first wiring pattern maptocylic is defined to take D1 elements, and for 1iD, each entry i of D1 elements is mapped to an entry log.g.(i). A second wiring pattern mapfromcylic is defined to do the opposite and map entry i to entry g.i.(mod D). Since 0 is not a member of the multiplicative group, make element entry 0 unaltered by both mapping and rotation. This structure is described in more detail below with respect to FIG. 6 ...displacer.6602600.The hardware implementation of the memory transfer operations described above includes the ability to permute data elements. A hardware device that does this is called a permuter. FIG. 6 is a block diagram 600 of a permutator 602 ..oO%DODrrDrglog.g..The reception parameter is considered in the case oO%D, where O is the offset start in the memory, which stores the data elements in an uninterrupted memory organized into D memory banks in the memory. Considering further that a parameter r is received, r denoting the number of rotations to be performed for cyclic groups smaller than D, where r is based at least on the discrete logarithm log.g.to the base g of the generator..604D6046061D1D.The permutator includes a number of inputs 604 to be permuted. These inputs will generally correspond to D inputs, usually memory banks. Those inputs 604 are then mapped from inputs indexed 1 to D to permutations via a first loop map 606 . In hardware, this operation can be implemented via a right barrel shifter that performs a cyclic right shift of the data elements from 1 to D..0D-16080D-1610.The permutator then permutes the data elements via a second cyclic map 608 that cyclically shifts all data elements from 0 to D-1 right. This may be performed via the right barrel shifter, which performs a cyclic right shift of the data elements from 0 to D1 and forwards the permuted data elements to output 610 in turn..0D-11D.Note that a reverse permutator can be implemented via a left barrel shifter that performs a cyclic left shift of data elements from 0 to D1, followed by a left barrel shifter that performs a cyclic left shift of data elements from 1 to D, which restores the data elements to their original positions. Thus, an inverse permuter is the hardware inverse of a permuter...Between the permuter and reverse permuter, hardware support for permutation operations for the memory transfer techniques disclosed herein is supported...Memory transfer optimization for neural network hardware acceleration.77027008700800.Permuters can be used to create memory transfer units optimized for neural network hardware acceleration. FIG. 7 is a block diagram 700 of an exemplary memory transfer unit 702 . FIG. 8 is a flowchart 800 of exemplary operations of the memory transfer unit 700 ..704or56800802800804704.The modulo address generator 704 is a forwarding or standard permutator. It receives input from a data bank, permuting data elements by input parameters o and r, as described above with respect to FIGS. 5 and 6 . Specifically, at block 802 of flowchart 800, the modulo address generator receives a memory address and a length, and at block 804 of flowchart 800, modulo address generator 704 generates a set of memory addresses corresponding to data elements stored in the stride-separated computer readable memory..706D708708800806706704800808708710712.The queuing controller 706 is responsible for controlling the forwarding of the received data elements to the D address queues 708 . Specifically, address queue 708 stores memory addresses of data elements in hardware, rather than the values of the data elements themselves. In block 806 of flowchart 800, queuing controller 706 receives the set of memory addresses generated by modulo address generator 704, and in block 808 of flowchart 800, forwards the memory addresses to the corresponding address queue 708 while simultaneously adding control information to control queue 710. The control information is used to synchronize reception on the receive dequeue controller 712 ..708714714716800810714708716.The address queues 708 respectively feed into a plurality of address decoders 714 which in turn feed into a plurality of data queues 716 respectively. Specifically, in block 810 of flowchart 800 , address decoder 714 decodes the memory addresses of the data elements in address queue 708 into their corresponding data elements and queues the decoded data elements into data queue 716 , respectively..800812710714708716.In block 812 of flowchart 800 , dequeue controller 710 receives queued data elements from data queue 714 and control information from control queue 708 and forwards to reverse permuter 716 based at least on the received control information..800814718704712800816720.In block 814 of flowchart 800 , reverse permutator 718 performs the reverse operation of the forward permutator of modulo address generator 704 to recover the data received from dequeue controller 712 . Once this is done, the recovered data is forwarded to data egress 720 in block 816 of flowchart 800 ...Exemplary use case.1.As described with respect to FIG. 1 , the multicast network and memory transfer optimizations disclosed herein can be applied to neural network operations. One or more multicast networks may be used to forward permutations of data elements stored in memory banks to multiple execution units. If multiple multicast units are used, those units may be fed into the execution units using different permutations. In one embodiment, different multicast units may refer to different partitions of tensors...In this way, the multicast network and the memory transfer unit can be conceived independently of and separately from the context of neural network hardware accelerators, each with applications that may not be relevant to neural network hardware acceleration...For example, multicast network and memory transfer optimizations can be used in other hardware acceleration scenarios such as graphics processing of computations including linear algebra operations, tensor operations and quaternion operations specific to graphics../.In the case of memory transfer optimization, the application need not be specific to a particular operation, but can only be used in the memory controller. For example, memory transfer optimization may be employed where partitions and/or vectors of data elements stored in non-stop memory are to be transferred. Furthermore, since multicast network optimizations can arbitrarily copy and permute data elements, use in conjunction with memory transfer operations can provide the basis for a full memory controller...in conclusion./.Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims."
}