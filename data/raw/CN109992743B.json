{
  "patent_id": "CN109992743B",
  "title": "Matrix multiplier",
  "assignee": "Huawei Technologies Co Ltd",
  "description": "Description.Matrix multiplier.Technical Field.The invention relates to the technical field of calculation, in particular to a matrix multiplier..Background.Currently, to calculate the product of two matrices a and B, the calculation can be performed in any one of the following two ways:.in the first mode, calculation is performed by a vector processor..Assuming that C is a  B and the number of elements that can be simultaneously calculated by the vector processor is M, referring to fig. 1, the vector processor will apply the ith row vector (including element a) of the matrix a.i1.A.i2.A.i(M-1).A.iM.) Load into source register Reg0, and then vector the jth column of matrix B (including element B).j1.B.j2.B.j(M-1).B.jM.) Loading the data into a register Reg1, realizing multiplication between corresponding elements of Reg0 and Reg1, and finally completing accumulation operation through an addition tree to calculate data C of ith row and jth column of matrix C.ij.And performing multiple calculations to obtain the matrix C..In the second mode, in order to further increase the calculation speed, the matrix multiplication can be performed by a two-dimensional calculation array..For example, the two-dimensional computational array may be a systolic array of N x N. In the first mode, completing two N x N matrix multiplications requires.N x.3 multiplications, since the vector processor can compute the multiplications between M elements every clock cycle, the time required to complete one multiplication is N x 3/M clock cycles. In the second method, N 3 multiplication operations are required to complete two N  N matrix multiplication operations, and since the systolic array has N 2 operation units, the time required for completing one matrix operation is N 3/N 2  N clock cycles. The first mode and the second mode both take long time to complete the matrix multiplication operation of N x N, and both have the problems of relatively fixed calculation size and inflexibility..Disclosure of Invention.The technical problem to be solved by the embodiments of the present invention is to provide a matrix multiplier and related devices, so as to solve the problems of inflexible calculation and low efficiency in matrix multiplication..In a first aspect, an embodiment of the present invention provides a matrix multiplier, which may include:.the first memory is used for storing a first matrix, and the first matrix is an M x K matrix;.the second memory is used for storing a second matrix, and the second matrix is a K x N matrix;.the operation circuit is connected with the first memory and the second memory and comprises X rows of operation units, Y columns of operation units, each operation unit comprises a vector multiplication circuit and an addition circuit, and the matrix multiplication circuit is used for receiving row vector data sent by the first memory and column vector data sent by the second memory and multiplying the two paths of vectors; the addition circuit is used for adding the multiplication results of the two paths of vectors and accumulating the calculation results belonging to the same operation unit to obtain the operation result of each operation unit;.a controller coupled to the arithmetic circuitry, the controller configured to:.partitioning the first matrix by taking the subblocks with the size of X L as a unit to obtain S multiplied by R subblocks with the same size, wherein the subblocks in the S th row and the R th column of the S multiplied by R subblocks are marked as A.sr.s(123S)r(123R).Partitioning the second matrix by using subblocks with the size of L x Y as a unit to obtain R x T subblocks with the same size, wherein the R row and the T column in the R x T subblocks are marked as B.rt.r(123R)t(123T).The controller is further configured to perform the following actions:.any one of the sub-blocks A.sr.X-th row of the X row vectors of (a) and the corresponding sub-block B.rt.The Y-th column of the Y column vectors is inputted to the arithmetic units of the X-th row and the Y-th column of the X-Y column arithmetic units, and X is (1, 2, 3,   X) and Y is (1, 2, 3,   Y), wherein any one of the sub-blocks a is a sub-block.sr.R and the corresponding sub-block B.rt.R in (a) are equal in value..In the embodiment of the present invention, a matrix multiplier is provided, which uses a controller to complete a block division method of matrix multiplication, i.e. MNK fractal, and divides a large matrix into unit matrix multiplication (i.e. matrix of X  L  Y) by using control logic of an internal controller 604 in a matrix multiplier 60. The control logic of controller 604 sends the unit matrix multiply task to the arithmetic circuitry 603 every clock cycle, so that the data is pipelined and the X rows by Y columns of arithmetic units are fully operational. The efficiency of matrix multiplication is improved, and the application effect of the neural network algorithm is obviously improved. The matrix multiplier provided by the embodiment of the invention can perform convolution operation and FC operation in a convolution neural network..In one possible implementation, the controller is specifically configured to perform the following actions:.any one of the sub-blocks A.sr.X-th row of the X row vectors of (a) and the corresponding sub-block B.rt.The Y-th column in the Y column vectors is input to the arithmetic units of the X-th row and the Y-th column in the X-row-Y-column arithmetic units in parallel in the same clock cycle for operation..In a possible implementation manner, the controller is further configured to control any one of a.sr.The row vectors enter the X-th row corresponding to the X-row-Y-column arithmetic units in sequence from small to large according to the number of the X rows, and the time difference of the adjacent row vectors entering the arithmetic units in different rows in the same column is 1 clock cycle; the controller is also used for simultaneously controlling the corresponding sub-blocks B.rt.The column vectors of (1) enter the Y-th row corresponding to the X row and Y column arithmetic units in sequence from small to large according to the Y column numbers, and the time difference of the adjacent column vectors entering the arithmetic units in different columns of the same row is 1 clock cycle..In one possible implementation, the controller is further configured to control:.in at least two continuous sub-block multiplication calculation periods, the values of s and r are unchanged, and the value of t is changed, so that the first memory multiplexes the same A in the at least two continuous sub-block multiplication calculation periods.sr.Wherein, the sub-block multiplication cycle is to complete the calculation of a sub-block A for the X rows by Y columns of operation units.sr.And the corresponding sub-block B.rt.The time taken for the matrix multiplication operation of (a)..In one possible implementation, the matrix multiplier further includes a third memory connected to the arithmetic circuit;.the controller is configured to control the X row X Y column arithmetic unit to store the calculation results of the vector multiplication circuit and the addition circuit in the third memory..In a possible implementation manner, the matrix multiplier further includes a fourth memory connected to the first memory and the second memory, and a fifth memory connected to the third memory;.the controller is further configured to control, prior to calculating the multiplication of the first matrix and the second matrix:.transporting the data sources of the first and second matrices from the fourth memory to the first and second memories, respectively; and the calculation result is transferred from the third memory to the fifth memory..In one possible implementation, the vector multiplication circuit includes L multipliers; the addition circuit comprises an addition tree with the input number of (L  1)..In one possible implementation, the first memory, the second memory, the arithmetic circuit, and the controller are connected through a bus interface unit..In one possible implementation form of the method,.when M% X is not equal to 0, all rows from (M 1) to (S X-M) of the first matrix are not calculated, and the result is assigned to 0, and when K% Y is not equal to 0, all rows from (K 1) to (R X Y-K) of the first matrix are not calculated, and the result is assigned to 0;.in one possible implementation form of the method,.when K% Y  0, (K 1) th through (R  Y-K) th rows of the first matrix are not calculated and the result is assigned to 0, and when N% X  0, (N 1) th through (T  X-N) th rows of the first matrix are not calculated and the result is assigned to 0..In a possible implementation manner, the matrix multiplier further includes a data handling unit, and the data handling unit is configured to perform an operation of transposing the first matrix before the first matrix is handled to the first storage, or perform an operation of transposing the second matrix before the second matrix is handled to the second storage..In one possible implementation, the controller controls any one of the sub-blocks of the first matrix to be stored in the first memory in a row form, or controls any one of the sub-blocks of the second matrix to be stored in the second memory in a row form. So as to be read out quickly when reading, and is more flexible and quicker when the sub-blocks are transposed..In a second aspect, the present application provides an electronic device, which may include:.the secure element provided by any one of the implementations of the first aspect above, and a discrete device coupled to the chip..In a third aspect, the present application provides a system-on-chip, including the chip provided in any one of the implementations of the first aspect. The SOC chip can be composed of chips, and can also comprise chips and other discrete devices.Drawings.In order to more clearly illustrate the technical solutions in the embodiments or the background art of the present invention, the drawings required to be used in the embodiments or the background art of the present invention will be described below..FIG. 1 is a schematic diagram of a prior art process for calculating the product of two matrices;.FIG. 2 is a diagram illustrating a prior art method for transforming convolution kernels into weight matrices;.FIG. 3 is a diagram illustrating a prior art method for converting input data into an input matrix;.FIG. 4 is a diagram illustrating a method for performing multiplication operations on two matrices in the prior art;.FIG. 5 is a schematic diagram of a prior art TPU pulse array;.FIG. 6 is a block diagram of a matrix multiplication accelerator according to an embodiment of the present invention.Fig. 7 is a structural diagram of an.arithmetic unit.6030 according to an embodiment of the present invention;.fig. 8 is a schematic diagram of a matrix block according to an embodiment of the present invention;.fig. 9 is a schematic diagram of a specific wiring in the arithmetic circuit 603 according to an embodiment of the present invention;.fig. 10 is a schematic diagram of a wiring in another specific operational circuit 603 according to an embodiment of the invention;.FIG. 11 is an input format of a.Base.4 matrix multiplier provided by an embodiment of the present invention;.fig. 12 is a schematic diagram of a pipelined implementation of the matrix multiplier with M-2N-2K-2 at time T-0;.fig. 13 is a schematic diagram of a pipelined implementation of the matrix multiplier with M-2N-2K-2 at time T-1;.fig. 14 is a schematic diagram of a pipelined implementation of the matrix multiplier with M-2N-2K-2 at time T-7;.fig. 15 is a schematic diagram of a pipelined implementation of the matrix multiplier with M-2N-2K-2 at time T-11;.FIG. 16 is a schematic diagram of another matrix multiplier according to an embodiment of the present invention;.fig. 17 is a schematic structural diagram of another matrix multiplier provided in the embodiment of the present invention;.FIG. 18 is a block diagram illustrating an asynchronous execution sequence of instructions according to an embodiment of the present invention..Detailed Description.The embodiments of the present invention will be described below with reference to the drawings..The terms \"first,\" \"second,\" \"third,\" and \"fourth,\" etc. in the description and claims of this application and in the accompanying drawings are used for distinguishing between different objects and not for describing a particular order. Furthermore, the terms \"include\" and \"have,\" as well as any variations thereof, are intended to cover non-exclusive inclusions. For example, a process, method, system, article, or apparatus that comprises a list of steps or elements is not limited to only those steps or elements listed, but may alternatively include other steps or elements not listed, or inherent to such process, method, article, or apparatus..Reference herein to \"an embodiment\" means that a particular feature, structure, or characteristic described in connection with the embodiment can be included in at least one embodiment of the application. The appearances of the phrase in various places in the specification are not necessarily all referring to the same embodiment, nor are separate or alternative embodiments mutually exclusive of other embodiments. It is explicitly and implicitly understood by one skilled in the art that the embodiments described herein can be combined with other embodiments..As used in this specification, the terms \"component,\" \"module,\" \"system,\" and the like are intended to refer to a computer-related entity, either hardware, firmware, a combination of hardware and software, or software in execution. For example, a component may be, but is not limited to being, a process running on a processor, an object, an executable, a thread of execution, a program, and/or a computer. By way of illustration, both an application running on a computing device and the computing device can be a component. One or more components can reside within a process and/or thread of execution and a component can be localized on one computer and/or distributed between 2 or more computers. In addition, these components can execute from various computer readable media having various data structures stored thereon. The components may communicate by way of local and/or remote processes such as in accordance with a signal having one or more data packets (e.g., data from two components interacting with another component in a local system, distributed system, and/or across a network such as the internet with other systems by way of the signal)..Secondly, the technical problems and application scenarios to be solved by the application are provided. In recent years, convolutional neural networks have become popular in research and development in academia and industry due to their unsophisticated manifestations in image classification, image recognition, audio recognition and other related fields. Convolutional neural networks mainly include convolutional and Fully Connected (FC) operations, wherein the operation amount of the convolutional operation can usually occupy more than 70% of the operation amount of the whole network..Convolution operations are not strictly equivalent to matrix multiplication operations, but can be converted to matrix multiplication operations by reasonable data adjustment. In a convolutional neural network, there are usually a plurality of convolution kernels, the convolution kernels are three-dimensional and contain data in three dimensions, the x and y directions are the length and width of the data, and the z direction can be regarded as the depth of the data. The convolution kernel is actually a filter (filter) and is mainly used for extracting different features in an image. Referring to fig. 2, the convolution kernel is substantially a combination of a series of weights, assuming that the number of convolution kernels is K, N elements in the z direction at the same position in the K convolution kernels are extracted, that is, a weight matrix (weight matrix) of N  K is obtained, and according to the specification of the matrix multiplier (i.e., the number of rows and columns of the matrix that can be calculated by the matrix multiplier), the convolution kernels may be stored in advance in the memory of the matrix multiplier in the form of the weight matrix so as to be called when the matrix multiplier performs a matrix multiplication operation. In the embodiment of the present invention, \"' means\" multiplication \"..Referring to fig. 3, according to the stride (stride) of the convolution kernel (in the embodiment of the present invention, the stride is 1), the matrix multiplier may extract N data of M points in the z direction, which are input, and may form an input matrix (input matrix), and the matrix multiplier needs to perform a multiplication operation on the input matrix and the weight matrix..The FC operation is essentially a vector by matrix multiplication. The input of the FC operation is a 9216 vector, and the FC needs to output 4096 points, then to obtain one point of the FC output, a 9126 vector and 9216 weights need to perform a dot-product operation, and to obtain all 4096 points, a 9216 vector and 9216x4096 weights need to perform a dot-product operation. Fig. 4 shows a calculation formula of a matrix C  a  B, where a is a matrix with size M  K and B is a matrix with size K  N, and in the present embodiment, M, N and K are both positive integers. To obtain one data in the C matrix by calculation, the data in one row vector in the matrix a and the corresponding data in one column vector in the matrix B are accumulated after dot product operation, that is, to obtain one data in the C matrix by calculation, N times of multiplication operation are performed, and M  N  K times of multiplication operation are performed on the matrix C by calculation..In the prior art, a systolic Array computing method, such as Google customized for machine learning application specific chip (ASIC) Google TPUv1, TPU uses systolic Array design, and implements a 256  256 2-D MAC Array for optimizing matrix multiplication and convolution operations (as shown in fig. 5). Each Cell in the figure is a multiplier, and after the multiplier multiplies an element in two matrices, a calculated result (Partial Sum, i.e. an intermediate result in matrix multiplication) is transmitted to an accumulation unit below the figure and accumulated with a previous associated accumulated value. Thus, when the data is running at full capacity, the systolic array accumulates one matrix-sized intermediate value every clock cycle. In the scheme, the matrix multiplication has low calculation efficiency due to low calculation density; secondly, during convolution operation, because the calculation size of the systolic array is relatively fixed, in order to reflect the operation efficiency of the systolic array, the input and the weight need to be converted in many forms, so that the operation is not flexible; furthermore, when performing matrix multiplication, the data needs to be large in size to achieve the effect of pipelining, for example, a 256  256 2D systolic array is not computationally efficient on a small matrix..In addition, the related patent realizes a 3-D MAC Array of M K N, and compared with a 2-D MAC Array scheme of TPUv1 and NVDLA, the matrix multiplication efficiency is further improved remarkably. The invention provides a novel hardware accelerator architecture, which can complete a matrix multiplication operation of [ NxN ] in a single clock cycle. In the hardware architecture, the number of processing units (PE) included is NxN, and the number of adder trees included is NxN. Meanwhile, a calculation method for splitting a large matrix into smaller matrices is also provided. But since the above scheme requires the matrix size to be padded to the size supported by the hardware, the data bandwidth is wasted and the computational efficiency is reduced. If the matrix is manually split into a large matrix and a small matrix, software programming is complex, and the relative software programming amount is also greatly increased. The accelerator can only load elements in the matrix in a unidirectional circulation mode and needs software to automatically split the matrix, so that the calculation mode is single and inflexible; furthermore, once the memories of matrix A and matrix B are not filled with all data, repeated reads may occur. The buffer size will depend strongly on the business algorithm, i.e. the accelerator will depend heavily on the tightly coupled on-chip memory..Therefore, the technical problem to be solved by the present application is how to use hardware to efficiently, flexibly and with low energy consumption to calculate a large amount of data operations in a convolutional neural network..It can be understood that the matrix multiplier provided by the embodiment of the present invention can be applied not only to the fields of machine learning, deep learning, convolutional neural network, etc., but also to the fields of digital image processing, digital signal processing, etc., and can also be applied to other fields related to matrix multiplication..Based on the above analysis, the present application provides a matrix multiplication accelerator that specifically analyzes and solves the technical problems presented in the present application. Referring to fig. 6, fig. 6 is a structural diagram of a matrix multiplication accelerator according to an embodiment of the present invention, and as shown in fig. 6, a matrix multiplier 60 includes: a first memory 601, a second memory 602, an arithmetic circuit 603, and a controller 604, wherein the arithmetic circuit 603 and the first memory 601, the second memory 602, and the controller 604 can communicate data via a bus. The operation circuit 603 is configured to extract matrix data in the first memory 601 and the second memory 602 and perform multiplication and addition of vectors, and the controller 604 is configured to control the operation circuit 603 to complete operation of vectors according to a preset program or instruction. Wherein,.the first memory 601 is used for storing a first matrix. The first matrix is an M  K matrix, and if the matrix a is the first matrix, the elements in the ith row and the jth column in the first matrix a may be denoted as a.ij.i(123M)j(123K).The first memory 601, the second memory 602, the.third memory.606 and the internal memory of the related matrix multiplier mentioned in the embodiments of the present invention may be registers, Random Access Memories (RAMs), static random access memories (srams), flash memories or other readable and writable memories. In the present application, the data types of the first matrix, the second matrix, and the operation result may be int 8, fp16, fp32, or the like..The second memory 602 is configured to store a second matrix, which is a K  N matrix. If the matrix B is a second matrix, the element in the jth row and the gth column in the second matrix B can be denoted as B.jg.j(123K)g(123N).Wherein M, K and N, and X and Y are both integers greater than 0, any two parameters of M, N and K may be equal or different, M, N and K may be equal or different, and X and Y may be equal or different, which is not specifically limited in this application..The arithmetic circuit 603 may include X row by Y column arithmetic units 6030 (may be referred to as multiply-accumulate units MAC), each of which may independently perform vector multiplication, and in fig. 6, the arithmetic circuit 603 includes 4 by 4 arithmetic units 6031, for example, so as to perform rendering, that is, X is 4 and Y is 4. The.operation unit.6030 has two inputs, which are respectively used to receive the row vector and the column vector sent by the first memory 601 and the second memory 602, and perform vector multiplication on the row vector and the column vector. Specifically, one.operation unit.6030 includes a vector multiplication circuit and an addition circuit, where the matrix multiplication circuit is configured to receive row vector data sent by the first memory 601 and column vector data sent by the second memory 602, and multiply the two paths of vectors; the addition circuit is used for adding the multiplication results of the two paths of vectors and accumulating the calculation results belonging to the same operation unit to obtain the calculation result of each operation unit..Referring to fig. 7, a block diagram of an.arithmetic unit.6030 is shown, and in one possible implementation, the vector multiplication circuit includes L (e.g., L  4) multipliers; the addition circuit comprises an addition tree with the input number of (L 1), namely the addition tree is used for accumulating L multiplication results and calculation results of the operation unit in different clock cycles. Optionally, the matrix multiplier 60 further comprises a.third memory.605, and the.third memory.605 is used for storing the operation results of the vector multiplication circuit and the addition circuit, and different clock cycles. It is understood that the.third memory.605 in the present application may include X  Y memory cells, each memory cell being used for storing the operation result of the corresponding operation unit each time. Or each operation unit corresponds to a designated storage space in the.third memory.605, and is used for storing each operation result..The controller 604 may calculate the product of the first matrix and the second matrix by performing the following actions:.the controller 604 divides the first matrix into blocks by using sub-blocks with the size of X  L as a unit to obtain S  R sub-blocks with the same size, wherein the sub-block in the S-th row and the R-th column of the S  R sub-blocks is denoted as a.sr.S is (1, 2, 3,   S) and R is (1, 2, 3,   R). That is, the matrix multiplier 60 in the present application is fixed in the matrix data of X  Y columns included therein as long as it is produced or shipped from the factory, and the number L of multipliers in the corresponding multiplier circuits is also fixed, so that it is necessary to perform matrix operation by fractal the first matrix and the second matrix, that is, matrix divisionAnd (5) partitioning. The division is performed by partitioning the first matrix into X  L sub-blocks. In the embodiment of the invention, the purpose of partitioning is to divide a large matrix into a plurality of small matrices which accord with the size of a matrix multiplier, then calculate the small matrices in a certain sequence and accumulate the values of the related small matrices to finally obtain a matrix multiplication result. The method can be used for flexibly calculating, is convenient for subsequent multiplexing and multilevel caching, further improves the calculation efficiency and reduces the data carrying bandwidth and energy consumption..It should be noted that the first matrix is an M  K matrix, and there may be a case where the first matrix cannot be exactly divided by X  L subblocks by an integer number. Therefore, when M/X or K/L is not an integer, the operation can be performed so that 0 elements are filled and filled. Or, the result is assigned to 0 without participating in the calculation at the corresponding position. In particular, the amount of the solvent to be used,.when M% X is not equal to 0, the (M 1) th row to the (S X X-M) th row of the first matrix are not calculated and the result is assigned to 0; and when K% Y is not equal to 0, the (K 1) th row to the R x Y-K th row of the first matrix are not calculated, and the result is assigned to be 0. That is, the corresponding rows and columns are not substantially multiplied by the corresponding arithmetic units, but are processed as having been operated but having a result of 0, so that the reading and operation power consumption of the corresponding arithmetic units can be saved..Correspondingly, the controller 604 further controls the second matrix to be partitioned into R  T sub-blocks of the same size by using the sub-blocks of the size L  Y as a unit, where R row and T column in the R  T sub-blocks are denoted as B.rt.R is (1, 2, 3,   R) and T is (1, 2, 3,   T). After the controller 604 controls the first matrix to be partitioned according to the specification of the arithmetic circuit 603, the second matrix must be correspondingly partitioned to match the first matrix, otherwise, the matrix multiplication cannot be performed..It should be noted that the second matrix is a K  N matrix, and there may be a case where the second matrix cannot be exactly divided by L  Y subblocks by an integer number. Thus, it is possible to provideWhen K/L or N/Y is not an integer, the operation can be performed so that 0 elements are filled and filled. Or, the result is assigned to 0 without participating in the calculation at the corresponding position. In particular, the amount of the solvent to be used,.when K% Y is not equal to 0, (K 1) th to (R x Y-K) th matrixes of the first matrix are not calculated and the result is assigned to 0; when N% X  0, the (N 1) th to (T X-N) th rows of the first matrix are not computed, and the result is assigned to 0. That is, the corresponding rows and columns are not substantially multiplied by the corresponding arithmetic units, but are processed as having been operated but having a result of 0, so that the reading and operation power consumption of the corresponding arithmetic units can be saved..After the first matrix and the second matrix are divided into blocks of fixed specifications, the blocks are input to the arithmetic circuit 603 to perform matrix multiplication between sub-blocks. In a specific calculation process, the controller 604 may control any one of the sub-blocks a in the first matrix.sr.X-th row of the X row vectors of (a) and the corresponding sub-block B.rt.The Y-th column of the Y column vectors is inputted to the arithmetic units of the X-th row and the Y-th column of the X-Y column arithmetic units, and X is (1, 2, 3,   X) and Y is (1, 2, 3,   Y), wherein any one of the sub-blocks a is a sub-block.sr.R and the corresponding sub-block B.rt.R in (a) are equal in value. Since sub-block A is now being used.sr.A row vector sum of B.rt.Before the column vectors of (a) are input to the arithmetic unit, matrix blocking, i.e. fractal, has been performed on the first matrix and the second matrix. Thus, in which order sub-block A is to be specifically ordered.sr.And corresponding B.rt.The input to the arithmetic circuit 603 can be implemented in various ways..In one possible implementation, the subblocks a may be respectively arranged.sr.And corresponding B.rt.The magnitude of s in (1) or the magnitude sequence of t is calculated in sequence. As shown in fig. 8, for example, the first matrix is M  K matrix, the second matrix is K  N, and it is assumed that M is 12, K is 6, and N is 12; x is 4, Y is 4, L is 3, and the first matrix and the second matrix areThe two matrices are partitioned to obtain S-3, R-2 and T-3. Thus obtaining a first matrix after blocking.Second matrix after blocking.And a is X L, i.e. a matrix of.4X.3, and each element in B is actually a matrix of L X Y, i.e..3X.4..In the multiplication of the first matrix and the second matrix, any one of the first matrix, i.e. each sub-block a, is required.sr.And the corresponding sub-block B in the second matrix.rt.And carrying out matrix multiplication operation of the sub-blocks. The determination of which sub-blocks to perform the matrix multiplication first, in which order, may include various embodiments,.in the first way, the order of matrix multiplication, for example, may be sub-block A.11.And sub-block B.11.Input A in the first sub-block multiplication cycle (which can be understood as the first round).11.All row vectors of (2) and corresponding B.11.All column vectors in (a) are operated on. A is performed again in the second sub-block multiplication period (which can be understood as the second round).12.All row vectors of (2) and corresponding B.21.All the column vectors in (a) are operated, so that, by accumulation of the operation units, the result point C of the first column in the first row of the result matrix C can be obtained.11.The value of (c). By analogy, the result points of all positions on the result matrix C can be obtained. And actually C.11.A.11.B.11.A.12.B.21.And wherein, in addition,.that is, C.11.In fact, the matrix C is 4  4, and therefore, according to the calculation rule of the matrix, the resulting matrix C is a result matrix of M  N, that is, a result matrix of 12  12..In the second mode, one of the sub-blocks is multiplexed according to a certain rule, and the embodiment of the invention provides a sub-block multiplexing mode for calling one sub-block a in the first matrix.sr.And the corresponding sub-block B in the second matrix.rt.And carrying out matrix multiplication operation of the sub-blocks. Specifically, the controller 604 is also used to control: in at least two continuous sub-block multiplication calculation periods, the values of s and r are unchanged, and the value of t is changed, so that the first memory multiplexes the same A in the at least two continuous sub-block multiplication calculation periods.sr.Wherein, the sub-block multiplication cycle is to complete the calculation of a sub-block A for the X rows by Y columns of operation units.sr.And the corresponding sub-block B.rt.The time taken for the matrix multiplication operation of (a)..For example, in the above assumption, M is 12, K is 6, and N is 12; in an embodiment where X is 4, Y is 4, and L is 3, in the first sub-block multiplication period (which may be understood as the first round), input a is input.11.And one of the corresponding sub-blocks B.11.All column vectors in (a) are operated on. In the second sub-block multiplication period (which can be understood as the second round), the values of s and r are kept unchanged, but t needs to be changed, namely A is carried out again.11.And another corresponding sub-block B.12.All column vectors in (a) are operated on. Optionally, A continues for a third sub-block multiply computation cycle (which may be understood as a third round).11.And a further corresponding sub-block B.13.All column vectors in (a) are operated on. Therefore, the A in the first storage can be reused in several adjacent sub-block multiplication calculation cycles.11.The read-write expenditure is saved, and the data carrying bandwidth is reduced..In the first and second modes, the sub-block a in the first matrix is selected from the sub-blocks a.sr.And a second momentCorresponding sub-block B in the array.rt.The calculation rule in one sub-block multiplication calculation period is that any one sub-block A in the first matrix.sr.X-th row of the X row vectors of (a) and the corresponding sub-block B.rt.The Y-th column of the Y column vectors is inputted to the arithmetic units of the X-th row and the Y-th column of the X-Y column arithmetic units, and X is (1, 2, 3,   X) and Y is (1, 2, 3,   Y), wherein any one of the sub-blocks a is a sub-block.sr.R and the corresponding sub-block B.rt.R in (a) are equal in value. That is, for A.sr.And the corresponding sub-block B in the second matrix.rt.Any row vector and any column vector are calculated by inputting to a designated arithmetic unit in the X row X Y column arithmetic units. For example, A.11.Second row vector [ a ] of.21.a.22.a.23.]And a sub-block B corresponding to one of the second matrices.11.Third column vector of.That is, the calculation is performed in the calculation unit corresponding to the 2 nd row and the 3 rd column in the calculation units of the X row and the Y column, and so on..Based on the arrangement of the operation units in the operation circuit 603 shown in fig. 6, referring to fig. 9, fig. 9 is a schematic diagram of a specific wiring in the operation circuit 603 according to an embodiment of the present invention..The BUFA is a first memory 601 of the first matrix, the BUFB is a second memory 602 of the second matrix, the BUFC is a.third memory.605 that stores the calculation results of the.operation units.6030, the operation circuit 603 includes X rows by Y columns (assuming that X is 4 and Y is 4) of operation units, that is, the MAC GRP R00C00 to MAC GRP R03C03 in the figure, and each operation unit MAC GRP may perform multiplication of one row vector of the X L matrix and one column of row vectors of the L Y matrix..The operation circuit 603, which may be referred to as a fractal matrix multiplication unit in the embodiment of the present invention, is composed of a 3-D MAC array (MAC Cube) and an Accumulator (Accumulator), and is configured to execute a fractal matrix multiplication instruction, as follows: c  a  B, or C  a  B  C, where a/B/C is a two-dimensional matrix. Wherein, the size of A is (M base) x (K base), the size of B is (K base) x (N base), and the size of C is (M base) x (N base). Base is the basic size of the computing circuit 603, i.e., X Y, such as 8X8, 16X 16, 32X 32, etc. The above calculation operation of C  a  B or C  a  B  C is called MNK matrix multiplication (and accumulation). In actual implementation, the matrix multiplication of MNK will be performed in a fractal manner, and the controller will control the decomposition of the large matrix into base-sized basic matrix multiplication and perform the matrix multiplication according to a specific order combination (the above-mentioned manner one or manner two)..The specific architecture of the fractal matrix multiplication unit is as shown in fig. 7 (assuming Base is 4), for example, in fig. 7, the MAC Group is a multiply-accumulate bank of N  N (4  4), and is composed of N (4) multiplication units and an N 1(5) input accumulation tree. At the matrix multiplication level, the multiply-accumulator may operate on a row by column and accumulate operations (i.e., one element in the result matrix). In fig. 9, there are a total of 4x4 multiply-accumulator sets, so that a complete 4x 4x 4x4 matrix multiplication can be calculated at the same time..It is understood that the layout of FIG. 9 can support the operation circuit 603 to complete a sub-block A in the same clock cycle.sr.And the corresponding sub-block B.rt.Is calculated by matrix multiplication. Because, A.sr.All X row vectors and the corresponding sub-block B.rt.All Y column vectors of (a) can simultaneously reach the.corresponding operation unit.6030 from the corresponding BUFA and BUFB through the wiring manner in fig. 9, and thus, the controller 604 can control the operation circuit 603 to complete one sub-block a in one clock cycle.sr.And the corresponding sub-block B.rt.In the next clock cycle, another sub-block A is completed.sr.And the corresponding sub-block B.rt.Or, the same sub-block A.sr.And a corresponding further sub-block B.rt.Is calculated by matrix multiplication..Referring to fig. 10, fig. 10 is a schematic diagram of a wiring in another specific operation circuit 603 according to an embodiment of the present invention. In the corresponding arithmetic circuit 603 of fig. 10, a systolic array structure is provided. Specifically, the controller 604 is used to control any one of A.sr.Row ofThe vectors sequentially enter the X-th row corresponding to the X-row-Y-column arithmetic units according to the sequence of the X-row numbers from small to large, and the time difference of the adjacent row vectors entering the arithmetic units in different rows in the same column is 1 clock cycle; the controller is also used for simultaneously controlling the corresponding sub-blocks B.rt.The column vectors of (1) enter the Y-th row corresponding to the X row and Y column arithmetic units in sequence from small to large according to the Y column numbers, and the time difference of the adjacent column vectors entering the arithmetic units in different columns of the same row is 1 clock cycle..That is, in order to fully utilize each operation unit 6030 (multiply-accumulator), the fractal matrix multiplication unit in the embodiment of the present invention may have a systolic array structure, which is different from the structure of TPUv1 in that the amount of data transferred per burst is L (and 1 in TPUv 1), and thus, the parallelism of data operations is greater than that of the systolic array in.TPUv.1..Based on the systolic array architecture, in the above-mentioned wiring structure corresponding to fig. 10, BUFA/B are memories for caching the first matrix/the second matrix, respectively, and in fig. 10, the first matrix cache device (BUFA) will divide the unit matrix in the a matrix into X rows and send L elements of the same row into one operation unit in the systolic array in sequence in each clock cycle. Similarly, a second matrix buffer device (BUFB) divides the identity matrix in the second matrix into Y columns and sequentially feeds the L elements of the same column into the systolic array every clock cycle. The specific time sequence is as follows:.the BUFC is a cache device (which can be built by an L0 cache or a cache register) for storing a 'C' (offset) matrix in 'A x B  C' calculation, and meanwhile, intermediate values in matrix multiplication can also be stored in the BUFC. After the multiply accumulator performs the multiplication, the accumulation tree accumulates the multiplied L intermediate values with the 1 offset or intermediate value stored in the BUFC..Taking M-2N-2K-2 (i.e., 8x 8x 8x8 matrix multiplication) as an example, the controller 603 in the matrix multiplier 60 splits the matrix multiplication into the format of fig. 11, which is a total of 8 unit matrix operations of.4x.4. For the matrix multiplication operation of the MNK, there are many possibilities of the splitting order, and the rule may be operated according to the above first mode and the second mode, which can be understood that the power consumption for reading data may be reduced by using the policy of the maximum multiplexing of data in the second mode. After MNK fractal splitting, the control logic of the controller 603 will transmit the 8 fractal into the systolic array in eight clock cycles, as shown in fig. 12 to fig. 15. Fig. 12 shows a pipelined execution of the fractal matrix multiplier with M-2N-2K-2 at time T-0, fig. 13 shows a pipelined execution of the fractal matrix multiplier with M-2N-2K-2 at time T-1, fig. 14 shows a pipelined execution of the fractal matrix multiplier with M-2N-2K-2 at time T-7, and fig. 15 shows a pipelined execution of the fractal matrix multiplier with M-2N-2K-2 at time T-11. It can be seen that at T-6, the 7 th clock cycle, the systolic array begins full load operation. In the last 6 clock cycles, the unit matrix fractal transmits out the pulse array, and the whole matrix also completes multiplication operation..Optionally, referring to fig. 16, the matrix multiplier 60 may further include an.instruction distributing unit.606, an.instruction fetching unit.607, a data carrying unit 608, a vector unit 609, a scalar unit 610, and a bus interface unit 611. Further, the matrix multiplier 60 provided in the embodiment of the present invention may be mounted as a coprocessor on a Central Processing Unit (CPU) 80, and the CPU allocates calculation tasks to the matrix multiplier 60, specifically, the CPU80 may store the first matrix and the second matrix and related instructions in the external memory 70, and the matrix multiplier 60 may complete matrix multiplication by reading the first matrix and the second matrix and related instructions in the external memory 70. The external Memory 70 may be a Double Data Rate Synchronous Dynamic Random Access Memory (DDR) or other readable and writable Memory, and the external Memory may be a Memory private to the matrix multiplier 60. Specifically, the first memory 601, the second memory 602, the.third memory.605 and the external memory 70 are all On-Chip (On-Chip) memories, wherein.1. Vector Unit 609(Vector Unit): contains various types of multi-parallelism computing devices (such as floating-point multiplication, floating-point addition, floating-point size comparison, etc.  ) for executing SIMD (single Instruction multiple data) instructions. And is responsible for direct data handling of Unified Buffer (Unified Buffer) and L0C caches (i.e., the first, second, and third memories)..2. Scalar Unit 610(Scalar Unit): with various types of shaping elementary arithmetic devices (e.g., add, multiply, compare, shift, etc.  )..3. A data transfer Unit (DMA Unit) configured to transfer data in each storage Unit, for example, data from the L1RAM to the L0RAM, where when matrix data involved in multiplication is transferred from an external Memory or an internal Memory of a matrix multiplier, the data transfer Unit in the embodiment of the present invention needs to store a matrix according to a result of blocking, for example, for a2  2 matrix, a sub-block in a first column of a first row in the first matrix.Storing in units of blocks, a0, a1, a2, A3 are stored as one row, and so on. Therefore, when the first matrix or the second matrix is conveyed to the corresponding first memory or the second matrix is conveyed to the corresponding second memory, the first matrix or the second matrix can be stored according to the above manner, and when the arithmetic unit needs to read, the arithmetic unit can also read according to the above storage sequence, so that when the row vector needs to be converted into the column vector during calculation, the transposition can be flexibly and quickly performed..4. An Instruction Fetch Unit 607 (issue Fetch Unit, IFU), i.e., an Instruction Fetch Unit, integrates a PC (program counter) and an IM (Instruction memory), fetches instructions from a main memory through a Bus Interface Unit (BIU)611, and decodes and controls the execution flow..5. The instruction Dispatch Unit 606(Dispatch Unit) parses the instruction transmitted by the immediate instruction fetch Unit, and submits the type instruction corresponding to the instruction to 4 pipeline units, where the pipeline units are a Scalar Unit (Scalar Unit), a data transfer (DMAUnit) Unit, a vector Unit (VectorUnit), and a fractal matrix multiplication Unit in fig. 16. The instruction dispatch unit has a mechanism to control the execution order-preserving among the 4 pipelines..It should be noted that the pipeline unit has two types, namely asynchronous Execution (post Execution) and synchronous Execution. All types of instructions are transmitted in order-preserving mode, and the difference is that the asynchronous execution unit executes instructions asynchronously and the synchronous execution unit finishes synchronously; wherein Scalar units (Scalar units) are executing in synchronization; the Fractal matrix multiplication Unit (fractional MatMult Unit), the DMA Unit and the Vector Unit (Vector Unit) are asynchronously executed..In a possible implementation manner, for the data carrying unit, the embodiment of the present invention provides a configurable random matrix transpose function. For example, when one of the block matrices of the first matrix is transferred from a certain memory (e.g., an external memory of the matrix multiplier) to another memory (e.g., an internal memory of the matrix multiplier), the data transfer unit performs an operation of transposing the matrix during the transfer, and stores the transposed matrix in the order of the transposed matrix. The matrix transposition is a necessary operation link of the neural network training process. Compared with the common instruction which is transferred after being transferred, the transfer instruction which can be transferred by the configured random matrix in the embodiment of the invention is more flexible, and the software is easier and simpler. As shown in the following table, the following,.general instructions: instructions for the configurable random matrix transpose function:.compared with a carrying instruction with a configurable random matrix transposition function, the common carrying instruction can support more application scenes under the condition that the same instruction has different parameters by supporting the configurable random matrix transposition function. A configurable random matrix transposition method suitable for a fractal matrix multiplication processor architecture is designed..Referring to fig. 17, in order to facilitate reuse of data, reduce power consumption, and reduce dependence on a tightly coupled on-chip memory, an embodiment of the present invention further provides a storage structure that adopts a multi-level cache. All the operation units can read and write the interactive data through a Unified Buffer. The matrix multiplier is internally provided with two levels of special caches L1 and L0. The L1 cache and the unified cache exchange data with an external storage space through a data handling DMA unit; the external storage space is composed of multi-level storage units. For example, the matrix multiplier includes multiple levels of caches, from L0, to L1, and then to L2, with increasing capacity, decreasing access bandwidth, increasing latency, and increasing power consumption overhead. L0 is the innermost cache, and can be used to cache three matrices of \"first matrix\", \"second matrix\", and \"result matrix\" in the MNK multiply instruction. Due to the close calculation, the requirements on bandwidth and delay are the highest, and the opportunity of data reuse is the largest. The D trigger (DFF) can be used for building, and the purposes of improving performance and reducing power consumption are achieved. The source and destination operands of the fractal instruction come from L1 (.fifth memory.612 and.fourth memory.613 in fig. 17). In execution the data is multiplexed by means of L0 (i.e. the first memory 601 and the second memory 602 in fig. 17). Software above the fractal instruction can reuse data via L1. By utilizing the execution sequence of the fractal instruction and the software control sequence above the fractal instruction, the reuse of the multi-level cache data can be realized. Meanwhile, by reusing the data of the multiple levels of caches, the data transmission time of the data in each cache can be covered. The following chart examples may illustrate data multiplexing and transport between multiple levels of buffering:.assume the following two matrices:.the data transfer procedure is shown in the following table:.time of day.Read L1.Into.L0.Computing.1.A0.B0.2.B1.A0B0.A0.B0.3.A2.AOB0B1.A0.B1.4.A1.A0A2B0B1.A2.B0.5.B2.A1A2B0B1.A2.B1.6.B3.A1A2B1B2.A1.B2.7.A3.A1A2B2B3.A1B3.8.A2A3B2B3.A3B2.9.A2A3B2B3.A3B3.At.time.1, the controller 60 reads the A0, B0 portion of the matrix from the L1 cache and stores it in the L0..At.time.2, the fractal matrices a0 and B0 can be read out from L0 and participate in the operation, and at this time, the hardware will simultaneously read out the fractal matrix B1 from L1 and store it in L0, so as to prepare for the next operation, and the data reading time will be masked by the calculation. In this case, the hardware does not read two fractal matrices at the same time, but only reads the B1 matrix. Because the matrix calculation of.time.3 \"a 0  B1\" multiplexes the data a0 stored at.time.1. Referring to the above list, it can be seen that in the following calculation, there is data multiplexing for each time unit..It should be noted that the embodiment of the present invention is not limited to the data transfer from L1 to L0, and the data transfer from L2 (for example, external memory 701 and external memory 702) to L1 cache may also use data reusability to achieve the purposes of reducing bandwidth and optimizing energy consumption. The embodiment of the invention does not limit the splitting mode and the carrying sequence of the matrix data, and the data carrying should maximize the data multiplexing so as to achieve that the fractal matrix calculation is operated at full load in each time unit..According to the embodiment of the invention, by the multi-level cache structure, the multi-level cache data reuse is realized by utilizing the data reuse of the matrix fractal, the execution sequence of the fractal instruction and the software control sequence above the fractal instruction, so that the dependence on a memory on a tight coupling chip is reduced, the energy efficiency is optimized, and the software programming complexity is reduced..In the embodiment of the invention, the execution sequence of the instruction for performing multiplication operation on the matrix comprises two modes of instruction synchronous execution and asynchronous execution:.in the embodiment of the invention, a series of control preparation and data preparation are required before the fractal matrix multiplication instruction is executed, such as: calculation of matrix size, reading of matrix data, calculation of target address, etc. If the instruction execution policy of the processor is synchronous execution, i.e., all instructions need to return in order (commit), the instructions will likely wait for the end of unassociated instructions to begin execution. This causes a large and unnecessary performance penalty, as follows: the execution order is synchronized for the instructions..Address calculation  control preparation .matrix.0 read .matrix.0 multiply  address calculation  control preparation .matrix.1 read .matrix.1 multiply..In the execution sequence, the second time of the control preparation of the.matrix.1, the address calculation and the data reading do not depend on the end of the multiplication of the.matrix.0, and the excessive time can cause unnecessary waiting time. To address this issue, in embodiments of the present invention, hardware.instruction dispatch unit.606 employs multi-channel order-preserving issue to ensure that different types of instructions can be executed simultaneously in sequence. As in the above example, control preparation and address calculation are performed in order within a scalar channel, matrix reading and storing are performed in order within a data transport channel, and matrix multiplication is also performed in order within a matrix operation channel. The channels can be overlapped with each other without order preservation, and the instructions which are mutually dependent can be synchronized by setting a waiting flag (WaitFlag). Through the strategy of asynchronous execution of the instructions, the instructions can be parallel, and the operation efficiency is improved. If the asynchronous execution strategy is adopted in the above example of the synchronous execution sequence, the effect is shown in fig. 18, in the instruction asynchronous execution sequence, the instructions are not order-preserved, and the instructions with dependency relationship can be synchronized by the waiting instruction added by the software. The control preparation overhead of the fractal matrix multiplication can be masked by adopting the asynchronous execution mode. An asynchronous execution mode suitable for a fractal matrix multiplication programming mode is designed..A matrix multiplier is provided that uses a controller to perform a block-wise approach to matrix multiplication, i.e., MNK fractal, by splitting a large matrix into identity matrix multiplications (i.e., a matrix of X L Y) by the control logic of an internal controller 604 in the matrix multiplier 60. The control logic of controller 604 sends the unit matrix multiply task to the arithmetic circuitry 603 every clock cycle, so that the data is pipelined and the X rows by Y columns of arithmetic units are fully operational. The efficiency of matrix multiplication is improved, and the application effect of the neural network algorithm is obviously improved. The matrix multiplier provided by the embodiment of the invention can perform convolution operation and FC operation in a convolution neural network..In the above embodiments, the implementation may be wholly or partially realized by software, hardware, firmware, or any combination thereof. When implemented using a software program, may be implemented in whole or in part in the form of a computer program product. The computer program product includes one or more computer instructions. The procedures or functions described in accordance with the embodiments of the present application are all or partially generated upon loading and execution of computer program instructions on a computer. The computer may be a general purpose computer, a special purpose computer, a network of computers, or other programmable device. The computer instructions may be stored on a computer readable storage medium or transmitted from one computer readable storage medium to another, for example, from one website, computer, server, or data center to another website, computer, server, or data center via wire (e.g., coaxial cable, fiber optic, Digital Subscriber Line (DSL)), or wireless (e.g., infrared, wireless, microwave, etc.). The computer-readable storage medium can be any available medium that can be accessed by a computer or can comprise one or more data storage devices, such as a server, a data center, etc., that can be integrated with the medium. The usable medium may be a magnetic medium (e.g., floppy Disk, hard Disk, magnetic tape), an optical medium (e.g., DVD), or a semiconductor medium (e.g., Solid State Disk (SSD)), among others..While the present application has been described in connection with various embodiments, other variations to the disclosed embodiments can be understood and effected by those skilled in the art in practicing the claimed application, from a review of the drawings, the disclosure, and the appended claims. In the claims, the word \"comprising\" does not exclude other elements or steps, and the word \"a\" or \"an\" does not exclude a plurality. A single processor or other unit may fulfill the functions of several items recited in the claims. The mere fact that certain measures are recited in mutually different dependent claims does not indicate that a combination of these measures cannot be used to advantage..Although the present application has been described in conjunction with specific features and embodiments thereof, it will be evident that various modifications and combinations can be made thereto without departing from the spirit and scope of the application. Accordingly, the specification and figures are merely exemplary of the present application as defined in the appended claims and are intended to cover any and all modifications, variations, combinations, or equivalents within the scope of the present application. It will be apparent to those skilled in the art that various changes and modifications may be made in the present application without departing from the spirit and scope of the application. Thus, if such modifications and variations of the present application fall within the scope of the claims of the present application and their equivalents, the present application is intended to include such modifications and variations as well."
}