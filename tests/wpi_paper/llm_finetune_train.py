# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
from sentence_transformers import SentenceTransformer
import random

import os
import sys

# app í´ë”ê°€ ìˆëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), "../..")))

from app.utils.llm.llm_factory import get_llm_client
from app.utils.llm.retry_utils import safe_invoke


def load_and_merge_data(old_path, new_path):
    df_old = pd.read_csv(old_path)
    df_new = pd.read_csv(new_path)
    df_all = pd.concat([df_old, df_new], ignore_index=True)
    return df_all

def majority_vote(row):
    votes = [row["fp_winner"], row["tu_winner"], row["sv_winner"]]
    vote_count = Counter(votes)
    top = vote_count.most_common(1)[0][1]
    top_winners = [k for k, v in vote_count.items() if v == top]
    return "tie" if len(top_winners) > 1 else top_winners[0]

def add_majority_vote_and_match(df):
    df["majority_winner"] = df.apply(majority_vote, axis=1)
    df["match"] = df["overall_winner"] == df["majority_winner"]
    return df

def add_dominant_reason(df, model_name="AI-Growth-Lab/PatentSBerta"):
    model = SentenceTransformer(model_name)

    fp_vecs = model.encode(df["fp_reason"].fillna("").tolist(), show_progress_bar=True)
    tu_vecs = model.encode(df["tu_reason"].fillna("").tolist(), show_progress_bar=True)
    sv_vecs = model.encode(df["sv_reason"].fillna("").tolist(), show_progress_bar=True)
    overall_vecs = model.encode(df["overall_reason"].fillna("").tolist(), show_progress_bar=True)

    FP_mean = np.mean(fp_vecs, axis=0)
    TU_mean = np.mean(tu_vecs, axis=0)
    SV_mean = np.mean(sv_vecs, axis=0)

    fp_sim = cosine_similarity(overall_vecs, [FP_mean]).flatten()
    tu_sim = cosine_similarity(overall_vecs, [TU_mean]).flatten()
    sv_sim = cosine_similarity(overall_vecs, [SV_mean]).flatten()

    dominant = []
    for i in range(len(fp_sim)):
        sim_dict = {"FP": fp_sim[i], "TU": tu_sim[i], "SV": sv_sim[i]}
        dominant.append(max(sim_dict, key=sim_dict.get))
    
    df["reason_dominant_aspect"] = dominant
    return df

def select_sv_bias_cases(df, seed=42, train_ratio=0.7):
    """
    SV dominant í¸í–¥ ì¼€ì´ìŠ¤ë¥¼ ì„ ë³„í•˜ê³ , í•™ìŠµ/ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

    ì¡°ê±´:
    1) match == True (majority winnerì™€ overall winnerê°€ ë™ì¼)
    2) fp_winner == tu_winner (ë‹¨, ë‘˜ ë‹¤ tie ì œì™¸)
    3) fp_winner âˆˆ ['ours', 'competitor']
    4) overall_winner == fp_winner
    5) dominant_reason == 'SV'
    """

    df_filtered = df[
        (df["match"] == True) &
        (df["fp_winner"] == df["tu_winner"]) &
        (df["fp_winner"].isin(["ours", "competitor"])) &
        (df["overall_winner"] == df["fp_winner"]) &
        (df["reason_dominant_aspect"] == "SV")
    ]

    # Dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    data_all = df_filtered.to_dict(orient="records")

    # ëœë¤ ì…”í”Œ ë° ë¶„í• 
    random.seed(seed)
    random.shuffle(data_all)
    split = int(len(data_all) * train_ratio)
    train_data = data_all[:split]
    valid_data = data_all[split:]

    return train_data, valid_data

def make_fp_tu_centered_prompt(entry):
    prompt = f"""
You are an AI patent evaluation assistant.

Given the following evaluations for Functional Purpose and Technical Uniqueness, explain why the final overall winner is **{entry['overall_winner']}**.

Use clear and concise technical language.
Do NOT mention Strategic Value, scalability, or business impact.

---
[Functional Purpose]
Winner: {entry['fp_winner']}
Our Patent: {entry['fp_ours']}
Competitor Patent: {entry['fp_competitor']}
Reason: {entry['fp_reason']}

[Technical Uniqueness]
Winner: {entry['tu_winner']}
Our Patent: {entry['tu_ours']}
Competitor Patent: {entry['tu_competitor']}
Reason: {entry['tu_reason']}

---
[Overall Reason]
""".strip()
    return prompt

# %%
def regenerate_overall_reason(train_data):
    updated_data = []
    llm = get_llm_client()

    for entry in train_data:    
        prompt = make_fp_tu_centered_prompt(entry)
        new_reason = safe_invoke(llm, prompt, parse_func=str)

        # ìƒˆ í•„ë“œë¡œ ì €ì¥ (ê¸°ì¡´ overall_reasonì€ ë³´ì¡´)
        entry['overall_reason_fp_tu'] = new_reason
        updated_data.append(entry)

    return updated_data

# %%
df_raw = load_and_merge_data("old/trial_full.csv", "new/trial_full.csv")
df_step1 = add_majority_vote_and_match(df_raw)
df_final = add_dominant_reason(df_step1)
train_data, valid_data = select_sv_bias_cases(df_final)

# %%
train_data = regenerate_overall_reason(train_data)

# %%
finetune_train_set = []
for e in train_data:
    finetune_train_set.append({
        "input": {
            "fp_winner": e["fp_winner"],
            "fp_reason": e["fp_reason"],
            "tu_winner": e["tu_winner"],
            "tu_reason": e["tu_reason"],
            "sv_winner": e["sv_winner"],
            "sv_reason": e["sv_reason"],
            "overall_winner": e["overall_winner"]
        },
        "output": {
            "overall_reason": e["overall_reason_fp_tu"]  # revised_reason ë“±
        }
    })

finetune_valid_set = []
for e in valid_data:
    finetune_valid_set.append({
        "input": {
            "fp_winner": e["fp_winner"],
            "fp_reason": e["fp_reason"],
            "tu_winner": e["tu_winner"],
            "tu_reason": e["tu_reason"],
            "sv_winner": e["sv_winner"],
            "sv_reason": e["sv_reason"],
            "overall_winner": e["overall_winner"]
        },
        "output": {
            "overall_reason": ""  # ê²€ì¦ ë°ì´í„°ëŠ” ì¶œë ¥ì´ ì—†ìŒ
        }
    })

import json

# ğŸ”¸ train.jsonlë¡œ ì €ì¥
with open("fine_tuned_train.jsonl", "w", encoding="utf-8") as f:
    for item in finetune_train_set:
        json.dump(item, f, ensure_ascii=False)
        f.write("\n")

# ğŸ”¹ valid.jsonlë¡œ ì €ì¥
with open("fine_tuned_valid.jsonl", "w", encoding="utf-8") as f:
    for item in finetune_valid_set:
        json.dump(item, f, ensure_ascii=False)
        f.write("\n")
# %%
import torch
print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))

# %%
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm
import os

# ğŸ“ ëª¨ë¸ ë¡œì»¬ ê²½ë¡œ
BASE_MODEL_PATH = "/mnt/d/hf_cache/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b"

# ğŸ“ ë°ì´í„° ë¡œë”©
dataset = load_dataset("json", data_files={
    "train": "fine_tuned_train.jsonl",
    "validation": "fine_tuned_valid.jsonl"
})

# ğŸ“ í† í¬ë‚˜ì´ì € ì„¤ì •
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True, use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# ğŸ“ í† í¬ë‚˜ì´ì¦ˆ í•¨ìˆ˜
def tokenize(example):
    input_ids_list = []
    attention_mask_list = []
    labels_list = []

    for i in range(len(example["input"])):
        input_ = example["input"][i]
        output_ = example["output"][i]

        prompt = (
            f"### Input:\n"
            f"fp_winner: {input_['fp_winner']}\n"
            f"fp_reason: {input_['fp_reason']}\n"
            f"tu_winner: {input_['tu_winner']}\n"
            f"tu_reason: {input_['tu_reason']}\n"
            f"sv_winner: {input_['sv_winner']}\n"
            f"sv_reason: {input_['sv_reason']}\n"
            f"overall_winner: {input_['overall_winner']}\n\n"
            f"### Output:\n"
        )
        answer = output_["overall_reason"]
        full_text = prompt + answer

        tokenized = tokenizer(
            full_text,
            truncation=True,
            padding="max_length",
            max_length=768
        )

        input_ids_list.append(tokenized["input_ids"])
        attention_mask_list.append(tokenized["attention_mask"])
        labels_list.append(tokenized["input_ids"])

    return {
        "input_ids": input_ids_list,
        "attention_mask": attention_mask_list,
        "labels": labels_list
    }

# ğŸ“ í† í°í™” ìˆ˜í–‰
tokenized_dataset = dataset.map(tokenize, remove_columns=["input", "output"], batched=True)
print("ğŸ”§ í† í°í™” ì™„ë£Œ")

# ğŸ“ BitsAndBytes 4bit config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# ğŸ“ Mistral-7B ëª¨ë¸ì„ 4bitë¡œ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_PATH,
    device_map="auto",
    quantization_config=bnb_config,
    trust_remote_code=True
)

# ğŸ“ LoRA êµ¬ì„±
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(base_model, lora_config)
print("âœ… ëª¨ë¸ + LoRA êµ¬ì„± ì™„ë£Œ")

# %%

# ğŸ“ ë°ì´í„°ë¡œë” êµ¬ì„±
from torch.utils.data import DataLoader

# âœ… collate_fn ì •ì˜: ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
def collate_fn(batch):
    input_ids = torch.tensor([x["input_ids"] for x in batch], dtype=torch.long)
    attention_mask = torch.tensor([x["attention_mask"] for x in batch], dtype=torch.long)
    labels = torch.tensor([x["labels"] for x in batch], dtype=torch.long)
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

# âœ… DataLoaderì— collate_fn ì ìš©
train_loader = DataLoader(tokenized_dataset["train"], batch_size=1, shuffle=True, collate_fn=collate_fn)
eval_loader = DataLoader(tokenized_dataset["validation"], batch_size=1, collate_fn=collate_fn)

print("ğŸ”§ ë°ì´í„°ë¡œë” êµ¬ì„± ì™„ë£Œ")

# ğŸ“ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
optimizer = AdamW(model.parameters(), lr=1e-4)

# ğŸ“ í•™ìŠµ ë£¨í”„
model.train()
for epoch in range(3):
    print(f"\nğŸ” Epoch {epoch + 1}")
    total_loss = 0
    for batch in tqdm(train_loader):
        input_ids = batch["input_ids"].to(model.device)
        attention_mask = batch["attention_mask"].to(model.device)
        labels = batch["labels"].to(model.device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    avg_loss = total_loss / len(train_loader)
    print(f"âœ… Epoch {epoch + 1} ì™„ë£Œ - í‰ê·  Loss: {avg_loss:.4f}")

# ğŸ“ ëª¨ë¸ ì €ì¥
output_dir = "./lora_output/mistral_lora_no_quant"
os.makedirs(output_dir, exist_ok=True)
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print("ğŸ’¾ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥ ì™„ë£Œ:", output_dir)

# ğŸ“ ë””ë°”ì´ìŠ¤ ë§¤í•‘ í™•ì¸
print("\nğŸ“ ë””ë°”ì´ìŠ¤ ë§¤í•‘ ê²°ê³¼")
for k, v in model.base_model.model.hf_device_map.items():
    print(f"{k:<60} â†’ {v}")