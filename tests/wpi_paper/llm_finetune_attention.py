# %%
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import AutoPeftModelForCausalLM 
import gc
import os

# âœ… ëª¨ë¸ ê²½ë¡œ
original_model_path = "/mnt/d/hf_cache/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b"
fine_tuned_model_path = "./lora_output/mistral_lora_no_quant"

# ì˜¤í”„ë¡œë“œ ê²½ë¡œ ì§€ì • (í•„ìš” ì‹œ í´ë” ì§ì ‘ ìƒì„±)
offload_path = "./offload_dir"
os.makedirs(offload_path, exist_ok=True)

# âœ… ì…ë ¥ ì˜ˆì‹œ
entry = {
    "fp_winner": "ours", 
    "fp_reason": "The first patent (US10231596B2) optimizes washing water distribution in the entire washing tank area, while the second patent (US11219349B2) only stabilizes rotation of wash arm assembly during dynamic operational conditions.", 
    "tu_winner": "ours", 
    "tu_reason": "The first patent (US10231596B2) utilizes a unique movable vane to distribute washing water, whereas the second patent (US11219349B2) uses a radial supporting portion in its mounting unit.", 
    "sv_winner": "ours", 
    "sv_reason": "The first patent (US10231596B2) improves cleaning effectiveness, potentially leading to customer satisfaction and market differentiation, while the second patent (US11219349B2) enhances product performance for consistent cleaning results.", 
    "overall_winner": "ours"
}

# âœ… í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def build_prompt(entry):
    return (
        f"### Input:\n"
        f"fp_winner: {entry['fp_winner']}\n"
        f"fp_reason: {entry['fp_reason']}\n"
        f"tu_winner: {entry['tu_winner']}\n"
        f"tu_reason: {entry['tu_reason']}\n"
        f"sv_winner: {entry['sv_winner']}\n"
        f"sv_reason: {entry['sv_reason']}\n"
        f"overall_winner: {entry['overall_winner']}\n\n"
        f"### Output:\n"
    )

# âœ… Attention ê³„ì‚° í•¨ìˆ˜
def get_attention_scores(model_path, model_name, input_ids, target_words, target_token_ids, use_lora=False):
    print(f"\nğŸš€ Loading {model_name} model ...")

    if use_lora:
        model = AutoPeftModelForCausalLM.from_pretrained(
            model_path,                     # LoRA adapter ê²½ë¡œ (ì—¬ê¸°ì— baseë„ í¬í•¨)
            torch_dtype=torch.float16,      # ë˜ëŠ” float32 ê°€ëŠ¥
            attn_implementation="eager",    # í•„ìš”ì‹œ ìœ ì§€
            output_attentions=True,
            device_map="auto",
            #offload_folder=offload_path,    # offload_dir ì•„ë‹˜! ì£¼ì˜
            low_cpu_mem_usage=False
        ).eval()
    else:
        model = AutoModelForCausalLM.from_pretrained(
            original_model_path,
            torch_dtype=torch.float16,             # ë˜ëŠ” float32
            attn_implementation="eager",
            output_attentions=True,
            device_map="auto",
            offload_folder=offload_path,
            low_cpu_mem_usage=True
        ).eval()

    with torch.no_grad():
        outputs = model(input_ids=input_ids, output_attentions=True)
        attentions = outputs.attentions

    if attentions is None:
        print(f"âŒ {model_name} returned no attention.")
        return {w: 0.0 for w in target_words}

    attn_map = torch.stack(attentions).mean(dim=0).mean(dim=1)[0]  # (seq_len, seq_len)

    result = {}
    for word, tid in zip(target_words, target_token_ids):
        try:
            token_positions = (input_ids[0] == tid).nonzero(as_tuple=True)[0]
            score = attn_map[:, token_positions].sum().item()
            result[word] = round(score, 4)
        except:
            result[word] = 0.0

    del model
    gc.collect()
    torch.cuda.empty_cache()
    print(f"âœ… {model_name} Attention:", result)
    return result

# âœ… Tokenizer ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained(original_model_path)
tokenizer.pad_token = tokenizer.eos_token

# âœ… ì…ë ¥ í† í°í™”
prompt = build_prompt(entry)
inputs = tokenizer(prompt, return_tensors="pt")
input_ids = inputs["input_ids"].to("cuda")

# âœ… ê´€ì‹¬ ë‹¨ì–´ ì„¤ì •
target_words = [
    "distribution", "washing", "tank",
    "movable", "vane", "radial", "supporting", "mounting", "unit",
    "customer", "satisfaction", "market", "differentiation", "performance", "effectiveness"
]
target_token_ids = [tokenizer.encode(w, add_special_tokens=False)[0] for w in target_words]

# âœ… ì›ë³¸ ëª¨ë¸ ì‹¤í–‰
scores_original = get_attention_scores(original_model_path, "Original", input_ids, target_words, target_token_ids)

# âœ… Fine-tuned ëª¨ë¸ ì‹¤í–‰ (LoRA ë³‘í•© í›„)
# scores_finetuned = get_attention_scores(fine_tuned_model_path, "Fine-tuned", input_ids, target_words, target_token_ids, use_lora=True)

# %%

# âœ… ë¹„êµ ê²°ê³¼ ì¶œë ¥
df = pd.DataFrame([scores_original, scores_finetuned], index=["Original", "Fine-tuned"])
print("\nğŸ“Š Attention Score Comparison:\n", df)
