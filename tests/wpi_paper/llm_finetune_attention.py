# %%
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import torch, torch.nn.functional as F
import pandas as pd, json, gc, os

base_model_path = "/mnt/d/hf_cache/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b"
lora_path = "./lora_output/mistral_lora_no_quant"
offload_path = "./offload_dir"
os.makedirs(offload_path, exist_ok=True)

# âœ… tokenizer: í™•ë¥  ì½”ë“œì™€ ë™ì¼
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token  # (í™•ë¥  ì½”ë“œì™€ ë™ì¼ ì„¸íŒ… ìœ ì§€)

# âœ… 4bit ì–‘ìí™”: í™•ë¥  ì½”ë“œì™€ ë™ì¼
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

def load_base_model():
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        device_map="auto",
        trust_remote_code=True,
        quantization_config=bnb_config,
        local_files_only=True,
        attn_implementation="eager"  # âœ… eager ê°•ì œ
    ).eval()
    model.config.output_attentions = True  # âœ… ì–´í…ì…˜ ë°˜í™˜ í™œì„±í™”
    return model

def load_finetuned_model():
    base_for_lora = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        device_map="auto",
        trust_remote_code=True,
        quantization_config=bnb_config,
        local_files_only=True,
        attn_implementation="eager"  # âœ… eager ê°•ì œ
    )
    base_for_lora.config.output_attentions = True  # âœ… ì–´í…ì…˜ ë°˜í™˜ í™œì„±í™”

    model = PeftModel.from_pretrained(
        base_for_lora,
        lora_path,
        is_trainable=False,
        local_files_only=True
    ).eval()
    model.config.output_attentions = True  # âœ… ì–´í…ì…˜ ë°˜í™˜ í™œì„±í™”
    return model

def build_prompt_shared(entry):
    input_ = entry["input"]
    return (
        f"Below is a comparison between two patents: Our patent and the Competitor's patent.\n\n"
        f"Functional Purpose:\n"
        f"- Winner: {input_['fp_winner']}\n"
        f"- Reason: {input_['fp_reason']}\n\n"
        f"Technical Uniqueness:\n"
        f"- Winner: {input_['tu_winner']}\n"
        f"- Reason: {input_['tu_reason']}\n\n"
        f"Strategic Value:\n"
        f"- Winner: {input_['sv_winner']}\n"
        f"- Reason: {input_['sv_reason']}\n\n"
        f"overall_winner: {input_['overall_winner']}\n\n"
        f"Write an overall judgement (1â€“2 sentences) justifying the overall winner.\n"
        f"Do not start with sentences like 'The overall winner is...'.\n"
        f"This explanation must reflect the actual aspect(s) that contributed to the win "
        f"(e.g., functional purpose, technical uniqueness, or strategic value).\n"
    )

# âœ… attention ê³„ì‚°: í™•ë¥  ì½”ë“œì™€ ë™ì¼í•œ generate ì„¤ì • + forwardì— attention_mask ëª…ì‹œ
def compute_attention_scores(model, prompt, target_words, target_token_ids_list):
    # í™•ë¥  ì½”ë“œì™€ ë™ì¼: ë§ˆìŠ¤í¬ ì—†ì´ generate ê°€ëŠ¥ (deterministic)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_len = inputs["input_ids"].shape[1]

    gen = model.generate(
        **inputs,
        max_new_tokens=150,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        return_dict_in_generate=True,
        output_scores=True
    )

    full_ids = gen.sequences  # [1, input+gen]
    gen_len = full_ids.shape[1] - input_len
    output_range = list(range(input_len, input_len + gen_len))

    # ê²°ê³¼ í…ìŠ¤íŠ¸ (í”„ë¡¬í”„íŠ¸ ì œì™¸)
    generated_text = tokenizer.decode(full_ids[0][input_len:], skip_special_tokens=True).strip()
    print("Generated:", generated_text[:200])

    # âš ï¸ ì—¬ê¸°ì„œë¶€í„°ê°€ 'attention' í•µì‹¬: forward ì¬ê³„ì‚° + attention_mask ëª…ì‹œ
    with torch.no_grad():
        full_attention_mask = torch.ones_like(full_ids).to(full_ids.device)  # íŒ¨ë”© ì—†ìŒ â†’ ì „ë¶€ 1
        outputs = model(
            input_ids=full_ids,
            attention_mask=full_attention_mask,
            output_attentions=True
        )
        # ë ˆì´ì–´, í—¤ë“œ í‰ê·  = í™•ë¥  ì½”ë“œì™€ ë…ë¦½ì  / ìš°ë¦¬ê°€ ë³´ê³  ì‹¶ì€ ê¸€ë¡œë²Œ ì§‘ì¤‘ë„
        attn_map = torch.stack(outputs.attentions).mean(dim=0).mean(dim=1)[0]  # (seq_len, seq_len)

    # í‚¤ì›Œë“œë³„ attention í‰ê· 
    result = {}
    for word, token_ids in zip(target_words, target_token_ids_list):
        token_positions = []
        for tid in token_ids:
            pos = (full_ids[0] == tid).nonzero(as_tuple=True)[0]
            token_positions.extend(pos.tolist())
        if token_positions:
            score = attn_map[output_range][:, token_positions].mean().item() # ì´ê²Œ í‰ê· ì´ ë˜ì–´ì•¼ í•¨
            result[word] = round(score, 4)
        else:
            result[word] = 0.0
    return result


# %% ì–´í…ì…˜ ì²˜ë¦¬ ë£¨í”„ í•¨ìˆ˜
def process_model(df_keywords, valid_entries, use_lora=False, tag="base"):
    
    if use_lora:
        model = load_finetuned_model()
    else:
        model = load_base_model()

    all_results = []

    def safe_split(val):
        return [w.strip() for w in str(val).split(",") if w.strip()] if pd.notna(val) else []

    for idx, entry in enumerate(valid_entries):
        print(f"Processing row {idx + 1}/{len(valid_entries)}")

        row = df_keywords.iloc[idx]

        # keyword ì „ì²´ê°€ NaNì´ë©´ skip
        if pd.isna(row.get("keyword")):
            continue

        fp_words = safe_split(row.get("fp_keyword"))
        tu_words = safe_split(row.get("tu_keyword"))
        sv_words = safe_split(row.get("sv_keyword"))
        all_words = fp_words + tu_words + sv_words
        token_ids_list = [tokenizer.encode(w, add_special_tokens=False) for w in all_words]

        # âœ… prompt ìƒì„± (entryëŠ” jsonlì—ì„œ ê°€ì ¸ì˜¨ ê²ƒ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        prompt = build_prompt_shared(entry)

        # âœ… ì–´í…ì…˜ ì ìˆ˜ ê³„ì‚°
        scores = compute_attention_scores(model, prompt, all_words, token_ids_list)

        def avg_score(keywords):
            vals = [scores[w] for w in keywords if w in scores]
            return round(sum(vals) / len(vals), 4) if vals else 0.0

        result = {
            "id": idx,
            f"{tag}_fp": avg_score(fp_words),
            f"{tag}_tu": avg_score(tu_words),
            f"{tag}_sv": avg_score(sv_words),
        }
        print(f"Results for row {idx + 1}: {result}")
        all_results.append(result)

    del model
    gc.collect()
    torch.cuda.empty_cache()
    return pd.DataFrame(all_results)

if __name__ == "__main__":
    import json
    import seaborn as sns
    import matplotlib.pyplot as plt

    # keyword ìˆëŠ” ê²ƒ ê°€ì§€ê³  ì˜¤ê¸°
    df_keywords = pd.read_csv("valid_output_base_finetuend_with_keyword.csv")
    df_keywords.columns = df_keywords.columns.str.strip()

    # Load valid entries from JSONL file
    with open("fine_tuned_valid.jsonl", "r", encoding="utf-8") as f:
        valid_entries = [json.loads(line) for line in f]

    # Run
    df_base = process_model(df_keywords, valid_entries, use_lora=False, tag="base")
    df_base.to_csv("attention_base.csv", index=False)

    # Step 2: Finetuned ëª¨ë¸ ë‚˜ì¤‘ì— ì²˜ë¦¬
    df_finetuned = process_model(df_keywords, valid_entries, use_lora=True, tag="finetuned")
    df_finetuned.to_csv("attention_finetuned.csv", index=False)

    # Step 3: ë‘ ê²°ê³¼ ë³‘í•© ë° ì €ì¥
    df_merged = pd.merge(df_base, df_finetuned, on="id")
    df_merged.to_csv("attention_comparison_final.csv", index=False)

    # ë³€í™”ëŸ‰ ê³„ì‚°
    delta_df = df_merged.copy()
    delta_df["delta_fp"] = delta_df["finetuned_fp"] - delta_df["base_fp"]
    delta_df["delta_tu"] = delta_df["finetuned_tu"] - delta_df["base_tu"]
    delta_df["delta_sv"] = delta_df["finetuned_sv"] - delta_df["base_sv"]

    # íˆíŠ¸ë§µìš© ë°ì´í„°
    heatmap_data = delta_df[["delta_fp", "delta_tu", "delta_sv"]]

    plt.figure(figsize=(6, 4))
    sns.heatmap(heatmap_data,  cmap="coolwarm", center=0, fmt=".2f")
    plt.title("Attention Change (Fine-tuned - Base)")
    plt.show()

    heatmap_data.mean()

    # ğŸ”¹ FP, TU, SV ê°ê°ì˜ Î”Attention ë¶„í¬ ê°€ì ¸ì˜¤ê¸°
    fp_deltas = delta_df['delta_fp'].dropna().values
    tu_deltas = delta_df['delta_tu'].dropna().values
    sv_deltas = delta_df['delta_sv'].dropna().values

    # ğŸ”¹ KDE Plot ì‹œê°í™”
    plt.figure(figsize=(10, 6))
    sns.kdeplot(fp_deltas, fill=True, label="FP", color="skyblue", linewidth=2)
    sns.kdeplot(tu_deltas, fill=True, label="TU", color="lightgreen", linewidth=2)
    sns.kdeplot(sv_deltas, fill=True, label="SV", color="salmon", linewidth=2)

    plt.axvline(0, linestyle='--', color='black', linewidth=1)
    plt.title("Attention Score Change Distribution per Reasoning Dimension (Î”Attention)")
    plt.xlabel("Attention Change (Fine-tuned â€“ Base)")
    plt.ylabel("Density")
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.3)
    plt.tight_layout()
    plt.show()
