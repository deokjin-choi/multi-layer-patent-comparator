# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
from sentence_transformers import SentenceTransformer
import random

import os
import sys

# app í´ë”ê°€ ìˆëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), "../..")))

from app.utils.llm.llm_factory import get_llm_client
from app.utils.llm.retry_utils import safe_invoke


#%%
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import torch
from tqdm import tqdm

# ğŸ“ 4bit ì–‘ìí™” config (baseìš©)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# ğŸ“ ê²½ë¡œ ì„¤ì •
fine_tuned_model_path = "./lora_output/mistral_lora_no_quant"
base_model_path = "/mnt/d/hf_cache/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b"
valid_file_path = "fine_tuned_valid.jsonl"
output_file_path = "predicted_valid_with_reason_short.jsonl"

# ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# ğŸ“ base model ë¨¼ì € ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=bnb_config,
    local_files_only=True
)

# %%
# ğŸ“ LoRA adapter ì ìš©
fine_tuned_model_path = "./lora_output/mistral_lora_no_quant"
model = PeftModel.from_pretrained(
    base_model,
    fine_tuned_model_path,
    is_trainable=False,
    local_files_only=True  # ì¶”ë¡  ìš©ë„ë©´ False
)
model.eval()

# ğŸ“ valid íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
with open(valid_file_path, "r", encoding="utf-8") as f:
    valid_entries = [json.loads(line) for line in f]

# ğŸ“ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def build_prompt(entry):
    input_ = entry["input"]
    prompt = (
        f"### Input:\n"
        f"fp_winner: {input_['fp_winner']}\n"
        f"fp_reason: {input_['fp_reason']}\n"
        f"tu_winner: {input_['tu_winner']}\n"
        f"tu_reason: {input_['tu_reason']}\n"
        f"sv_winner: {input_['sv_winner']}\n"
        f"sv_reason: {input_['sv_reason']}\n"
        f"overall_winner: {input_['overall_winner']}\n\n"
        f"### Output:\n"
        f"Write an overall judgement (1â€“2 sentences) justifying the overall winner. \n"
        f"Do not start with sentences like 'The overall winner is...'\n"
        f"This explanation must reflect the actual aspect(s) that contributed to the win (e.g., functional purpose, technical uniqueness, or strategic value).\n"
        
    )
    return prompt

# ğŸ“ ì¶”ë¡  ìˆ˜í–‰
results = []

for entry in tqdm(valid_entries):
    prompt = build_prompt(entry)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    input_length = inputs["input_ids"].shape[1]  # í”„ë¡¬í”„íŠ¸ í† í° ê¸¸ì´

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            do_sample=True,
            temperature=0.3,
            pad_token_id=tokenizer.eos_token_id
        )

    # í”„ë¡¬í”„íŠ¸ ì´í›„ ìƒì„±ëœ í† í°ë§Œ ì¶”ì¶œ
    generated_tokens = outputs[0][input_length:]
    predicted_reason = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()


    entry["output"]["overall_reason"] = predicted_reason
    print(f"Generated reason: {predicted_reason}")
    results.append(entry)


# ğŸ“ ê²°ê³¼ ì €ì¥
with open(output_file_path, "w", encoding="utf-8") as f:
    for item in results:
        json.dump(item, f, ensure_ascii=False)
        f.write("\n")

print(f"âœ… ì´ {len(results)}ê±´ì˜ ì˜ˆì¸¡ ì™„ë£Œ. ê²°ê³¼ íŒŒì¼: {output_file_path}")

# %%

import json
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
import numpy as np
from tqdm import tqdm

# 1.ì „ì²´ ë°ì´í„° ë¡œë”© ë° í•„ìš” í•„ë“œë§Œ ë‚¨ê¸°ê¸°
# df_total = load_and_merge_data("old/trial_full.csv", "new/trial_full.csv")
# df_total = df_total[["fp_reason", "tu_reason", "sv_reason"]]

# ğŸ“ ê²½ë¡œ
output_file_path = "predicted_valid_with_reason_short.jsonl"
embedding_model_name = "AI-Growth-Lab/PatentSBerta"

# ğŸ“ 2. valid íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
with open(output_file_path, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

# ğŸ“ 3. valid íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì •ë¦¬
rows = []
for item in data:
    row = {
        "fp_reason": item["input"]["fp_reason"],
        "tu_reason": item["input"]["tu_reason"],
        "sv_reason": item["input"]["sv_reason"],
        "overall_reason": item["output"]["overall_reason"]
    }
    rows.append(row)

df_valid = pd.DataFrame(rows)

# ğŸ“ 4. dominant aspect ê³„ì‚°
model = SentenceTransformer(embedding_model_name)

fp_vecs = model.encode(df_valid["fp_reason"].fillna("").tolist(), show_progress_bar=True)
tu_vecs = model.encode(df_valid["tu_reason"].fillna("").tolist(), show_progress_bar=True)
sv_vecs = model.encode(df_valid["sv_reason"].fillna("").tolist(), show_progress_bar=True)
overall_vecs = model.encode(df_valid["overall_reason"].fillna("").tolist(), show_progress_bar=True)

FP_mean = np.mean(fp_vecs, axis=0)
TU_mean = np.mean(tu_vecs, axis=0)
SV_mean = np.mean(sv_vecs, axis=0)

fp_sim = cosine_similarity(overall_vecs, [FP_mean]).flatten()
tu_sim = cosine_similarity(overall_vecs, [TU_mean]).flatten()
sv_sim = cosine_similarity(overall_vecs, [SV_mean]).flatten()

dominant = []
for i in range(len(fp_sim)):
    sim_dict = {"FP": fp_sim[i], "TU": tu_sim[i], "SV": sv_sim[i]}
    dominant.append(max(sim_dict, key=sim_dict.get))

df_valid["reason_dominant_aspect"] = dominant

# ğŸ“ 4. ë¹„ìœ¨ í†µê³„ ì¶œë ¥
count = Counter(dominant)
total = sum(count.values())

print("\nğŸ“Š dominant aspect ë¹„ìœ¨:")
for aspect in ["FP", "TU", "SV"]:
    pct = (count[aspect] / total) * 100
    print(f"  - {aspect}: {count[aspect]}ê±´ ({pct:.1f}%)")

# %%
# ğŸ“ 5. ì›ë³¸ JSONL ë°ì´í„°ì— dominant_aspect ì¶”ê°€
for i in range(len(data)):
    data[i]["dominant_aspect"] = df_valid.loc[i, "reason_dominant_aspect"]

# ğŸ“ 6. ê²°ê³¼ ë®ì–´ì“°ê¸° ì €ì¥
with open(output_file_path, "w", encoding="utf-8") as f:
    for item in data:
        json.dump(item, f, ensure_ascii=False)
        f.write("\n")

print(f"\nâœ… dominant_aspect ì¶”ê°€ ì™„ë£Œ ë° ì €ì¥: {output_file_path}")
